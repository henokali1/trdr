{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "from binance.client import Client\n",
    "from time import time\n",
    "import pickle as pickle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from pycaret.classification import load_model, predict_model\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def get_client():\n",
    "    fn = '../../key/binance-key.pickle'\n",
    "    # fn = '/home/era/key/binance-key.pickle'\n",
    "    with open(fn, 'rb') as handle:\n",
    "        k = pickle.load(handle)\n",
    "    return Client(k['API_KEY'], k['API_SECRET'])\n",
    "\n",
    "\n",
    "client = get_client()\n",
    "\n",
    "\n",
    "def get_unix_timestamp(date_string):\n",
    "    \"\"\"\n",
    "    Converts the input date string to Unix timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        date_string (str): Input date string in the format \"dd/mm/yyyy hh:mm:ss\".\n",
    "\n",
    "    Returns:\n",
    "        int: Unix timestamp of the given date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_string, \"%d/%m/%Y %H:%M:%S\")\n",
    "        timestamp = int(date_obj.timestamp())\n",
    "        return timestamp\n",
    "    except ValueError:\n",
    "        print(\"Invalid date format. Please use the format 'dd/mm/yyyy hh:mm:ss'.\")\n",
    "        return None\n",
    "\n",
    "def get_historical_data(start_timestamp, end_timestamp, coin_pair): \n",
    "    data = []\n",
    "    tot = (end_timestamp - start_timestamp)/(900*500)\n",
    "    cntr = 0\n",
    "    for current_sts in range(start_timestamp, end_timestamp+1, 900*500):\n",
    "        next_ets = current_sts + 900*500 if (current_sts + 900*500) < end_timestamp else end_timestamp\n",
    "        print(current_sts, next_ets, f'100% completed') if next_ets == end_timestamp else print(current_sts, next_ets, f'{round(cntr*100/tot, 1)}% completed')\n",
    "        cntr += 1\n",
    "        # Futures market\n",
    "        klines = client.futures_historical_klines(coin_pair, '15m', current_sts*1000, next_ets*1000, limit=500)\n",
    "        # Spot market\n",
    "        # klines = client.get_historical_klines(coin_pair, interval, current_sts*1000, next_ets*1000, limit=500)\n",
    "        \n",
    "        for kline in klines:\n",
    "            timestamp = kline[0]/1000\n",
    "            open_price = float(kline[1])\n",
    "            high_price = float(kline[2])\n",
    "            low_price = float(kline[3])\n",
    "            close_price = float(kline[4])\n",
    "            volume = float(kline[5])\n",
    "\n",
    "            data.append([timestamp, open_price, high_price, low_price, close_price, volume])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    # df.to_csv(f'/home/ubuntu/data/{coin_pair}-{interval}.csv', index=False)\n",
    "    # print('Data Exported')\n",
    "    print(f'Historical Data of {coin_pair} Downloaded')\n",
    "    return df\n",
    "\n",
    "def generate_features(start_timestamp, end_timestamp, coin_pair):\n",
    "    df = get_historical_data(start_timestamp, end_timestamp, coin_pair)\n",
    "    candlestick_frame = 12\n",
    "    pnl_threshold = 3\n",
    "\n",
    "\n",
    "    # df = pd.read_csv('/home/ubuntu/data/ETHUSDT-15m.csv')\n",
    "    try:\n",
    "        df.ta.strategy(\"all\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ts = list(df['time'])\n",
    "        open = list(df['open'])\n",
    "        high = list(df['high'])\n",
    "        low = list(df['low'])\n",
    "        close = list(df['close'])\n",
    "        volume = list(df['volume'])\n",
    "        tot = len(ts)\n",
    "        long_runup_lst = []\n",
    "        long_drawdown_lst = []\n",
    "        short_runup_lst = []\n",
    "        short_drawdown_lst = []\n",
    "\n",
    "        for idx in range(tot):\n",
    "            if (idx >= candlestick_frame) and (idx <= tot - candlestick_frame):\n",
    "                max_high = max(high[idx+1:idx+candlestick_frame])\n",
    "                min_low = min(low[idx+1:idx+candlestick_frame])\n",
    "                entry_price = open[idx+1]\n",
    "                long_runup_lst.append(round((max_high*100/entry_price)-100, 6))\n",
    "                long_drawdown_lst.append(round((min_low*100/entry_price)-100, 6))\n",
    "                short_runup_lst.append(round((entry_price*100/min_low)-100, 6))\n",
    "                short_drawdown_lst.append(round((entry_price*100/max_high)-100, 6))\n",
    "            else:\n",
    "                long_runup_lst.append(0)\n",
    "                long_drawdown_lst.append(0)\n",
    "                short_runup_lst.append(0)\n",
    "                short_drawdown_lst.append(0)     \n",
    "\n",
    "\n",
    "        long=[]\n",
    "        short=[]\n",
    "        dont_trade=[]\n",
    "        signal = []\n",
    "\n",
    "        for idx in range(tot):\n",
    "            if (idx >= candlestick_frame) and (idx <= tot - candlestick_frame):\n",
    "                if long_runup_lst[idx] >= pnl_threshold:\n",
    "                    signal.append('long')\n",
    "                elif short_runup_lst[idx] >= pnl_threshold:\n",
    "                    signal.append('short')\n",
    "                else:\n",
    "                    signal.append('dont_trade')\n",
    "            else:\n",
    "                signal.append('dont_trade')\n",
    "\n",
    "        df['coin'] = [coin_pair]*len(signal)\n",
    "        df['signal'] = signal\n",
    "\n",
    "        long_indices = df[df['signal'].str.contains('long', case=False)].index\n",
    "        short_indices = df[df['signal'].str.contains('short', case=False)].index\n",
    "        dont_trade_indices = list(df[df['signal'].str.contains('dont_trade', case=False)].index)\n",
    "        num_indices_to_pick  = len(dont_trade_indices) - min([len(long_indices), len(short_indices)])\n",
    "        random_indices = random.sample(dont_trade_indices, num_indices_to_pick)\n",
    "        df = df.drop(random_indices)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        df = df.drop(columns = ['time', 'open', 'high', 'low', 'close', 'volume'], axis=1)\n",
    "        dataset_fn = f'../../data/{coin_pair}-dataset.csv'\n",
    "        pd.DataFrame(df).to_csv(dataset_fn, index=False)\n",
    "        print(f'{coin_pair} Features Generated and saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins_list = ['BTC', 'ETH', 'BNB', 'XRP', 'ADA', 'DOGE', 'SOL', 'TRX', 'DOT', 'MATIC', 'LTC', 'BCH', 'AVAX', 'XLM', 'LINK', 'UNI', 'XMR', 'ATOM', 'ETC', 'HBAR', 'ICP', 'FIL', 'LDO', 'APT', 'ARB', 'QNT', 'VET', 'NEAR', 'OP', 'MKR', 'GRT', 'AAVE', 'ALGO', 'AXS', 'EGLD', 'STX', 'SAND', 'XTZ', 'EOS', 'INJ', 'THETA', 'IMX', 'SNX', 'MANA', 'FTM', 'RUNE', 'APE', 'RNDR', 'NEO', 'KAVA', 'FLOW', 'CHZ', 'GALA', 'KLAY', 'SUI', 'FXS', 'ZEC', 'CFX', 'CRV', 'MINA', 'COMP', 'GMX', 'DYDX', 'WOO', 'ASTR']\n",
    "for i in range(len(coins_list)):\n",
    "    coin_pair = f'{coins_list[i]}USDT'\n",
    "    print('working on', coin_pair)\n",
    "    start_timestamp = get_unix_timestamp('1/1/2016 00:00:00')\n",
    "    end_timestamp = int(time())\n",
    "    generate_features(start_timestamp, end_timestamp, coin_pair)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "\n",
    "dataset_path = '../../data/'\n",
    "all_datasets = [f for f in listdir(dataset_path) if isfile(join(dataset_path, f))]\n",
    "can_be_used_for_training = []\n",
    "for fn in all_datasets:\n",
    "    df = pd.read_csv(f'{dataset_path}{fn}')\n",
    "    dont_trade_count = list(df['signal']).count('dont_trade')\n",
    "    if dont_trade_count >= 9000:\n",
    "        can_be_used_for_training.append(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "coin_feaures = ['MATICUSDT-dataset.csv', 'EOSUSDT-dataset.csv', 'COMPUSDT-dataset.csv', 'DOGEUSDT-dataset.csv', 'MKRUSDT-dataset.csv', 'VETUSDT-dataset.csv', 'FILUSDT-dataset.csv', 'GALAUSDT-dataset.csv', 'LTCUSDT-dataset.csv', 'BCHUSDT-dataset.csv', 'SANDUSDT-dataset.csv', 'SOLUSDT-dataset.csv', 'DYDXUSDT-dataset.csv', 'FTMUSDT-dataset.csv', 'XLMUSDT-dataset.csv', 'UNIUSDT-dataset.csv', 'GRTUSDT-dataset.csv', 'LINKUSDT-dataset.csv', 'XTZUSDT-dataset.csv', 'SNXUSDT-dataset.csv', 'ZECUSDT-dataset.csv', 'AXSUSDT-dataset.csv', 'AVAXUSDT-dataset.csv', 'ADAUSDT-dataset.csv', 'EGLDUSDT-dataset.csv', 'ATOMUSDT-dataset.csv', 'MANAUSDT-dataset.csv', 'KAVAUSDT-dataset.csv', 'XRPUSDT-dataset.csv', 'NEOUSDT-dataset.csv', 'CHZUSDT-dataset.csv', 'ETCUSDT-dataset.csv', 'DOTUSDT-dataset.csv', 'RUNEUSDT-dataset.csv', 'ALGOUSDT-dataset.csv', 'AAVEUSDT-dataset.csv', 'NEARUSDT-dataset.csv', 'CRVUSDT-dataset.csv', 'THETAUSDT-dataset.csv', 'HBARUSDT-dataset.csv']\n",
    "dfs=[]\n",
    "dataset_path = '../../data/'\n",
    "\n",
    "for fn in coin_feaures:\n",
    "    dfs.append(pd.read_csv(f'{dataset_path}{fn}'))\n",
    "\n",
    "concatenated_df = pd.concat(dfs, ignore_index=True)  # Set ignore_index=True to reset index\n",
    "pd.DataFrame(concatenated_df).to_csv(f'{dataset_path}combined_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bd3b0_row11_col1, #T_bd3b0_row20_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bd3b0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bd3b0_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_bd3b0_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bd3b0_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_bd3b0_row0_col1\" class=\"data row0 col1\" >123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bd3b0_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_bd3b0_row1_col1\" class=\"data row1 col1\" >signal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_bd3b0_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_bd3b0_row2_col1\" class=\"data row2 col1\" >Multiclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_bd3b0_row3_col0\" class=\"data row3 col0\" >Target mapping</td>\n",
       "      <td id=\"T_bd3b0_row3_col1\" class=\"data row3 col1\" >dont_trade: 0, long: 1, short: 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_bd3b0_row4_col0\" class=\"data row4 col0\" >Original data shape</td>\n",
       "      <td id=\"T_bd3b0_row4_col1\" class=\"data row4 col1\" >(1678255, 211)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_bd3b0_row5_col0\" class=\"data row5 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_bd3b0_row5_col1\" class=\"data row5 col1\" >(1678255, 211)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_bd3b0_row6_col0\" class=\"data row6 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_bd3b0_row6_col1\" class=\"data row6 col1\" >(1174778, 211)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_bd3b0_row7_col0\" class=\"data row7 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_bd3b0_row7_col1\" class=\"data row7 col1\" >(503477, 211)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_bd3b0_row8_col0\" class=\"data row8 col0\" >Numeric features</td>\n",
       "      <td id=\"T_bd3b0_row8_col1\" class=\"data row8 col1\" >209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_bd3b0_row9_col0\" class=\"data row9 col0\" >Categorical features</td>\n",
       "      <td id=\"T_bd3b0_row9_col1\" class=\"data row9 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_bd3b0_row10_col0\" class=\"data row10 col0\" >Rows with missing values</td>\n",
       "      <td id=\"T_bd3b0_row10_col1\" class=\"data row10 col1\" >100.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_bd3b0_row11_col0\" class=\"data row11 col0\" >Preprocess</td>\n",
       "      <td id=\"T_bd3b0_row11_col1\" class=\"data row11 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_bd3b0_row12_col0\" class=\"data row12 col0\" >Imputation type</td>\n",
       "      <td id=\"T_bd3b0_row12_col1\" class=\"data row12 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_bd3b0_row13_col0\" class=\"data row13 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_bd3b0_row13_col1\" class=\"data row13 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_bd3b0_row14_col0\" class=\"data row14 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_bd3b0_row14_col1\" class=\"data row14 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_bd3b0_row15_col0\" class=\"data row15 col0\" >Maximum one-hot encoding</td>\n",
       "      <td id=\"T_bd3b0_row15_col1\" class=\"data row15 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_bd3b0_row16_col0\" class=\"data row16 col0\" >Encoding method</td>\n",
       "      <td id=\"T_bd3b0_row16_col1\" class=\"data row16 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_bd3b0_row17_col0\" class=\"data row17 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_bd3b0_row17_col1\" class=\"data row17 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_bd3b0_row18_col0\" class=\"data row18 col0\" >Fold Number</td>\n",
       "      <td id=\"T_bd3b0_row18_col1\" class=\"data row18 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_bd3b0_row19_col0\" class=\"data row19 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_bd3b0_row19_col1\" class=\"data row19 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_bd3b0_row20_col0\" class=\"data row20 col0\" >Use GPU</td>\n",
       "      <td id=\"T_bd3b0_row20_col1\" class=\"data row20 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_bd3b0_row21_col0\" class=\"data row21 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_bd3b0_row21_col1\" class=\"data row21 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_bd3b0_row22_col0\" class=\"data row22 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_bd3b0_row22_col1\" class=\"data row22 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd3b0_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_bd3b0_row23_col0\" class=\"data row23 col0\" >USI</td>\n",
       "      <td id=\"T_bd3b0_row23_col1\" class=\"data row23 col1\" >b9b4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff1f02ff5e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n",
      "[LightGBM] [Fatal] CUDA Tree Learner was not enabled in this build.\n",
      "Please recompile with CMake option -DUSE_CUDAP=1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_bd661_row10_col0, #T_bd661_row10_col1, #T_bd661_row10_col2, #T_bd661_row10_col3, #T_bd661_row10_col4, #T_bd661_row10_col5, #T_bd661_row10_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_bd661\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_bd661_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_bd661_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_bd661_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_bd661_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_bd661_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_bd661_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_bd661_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_bd661_row0_col0\" class=\"data row0 col0\" >0.8599</td>\n",
       "      <td id=\"T_bd661_row0_col1\" class=\"data row0 col1\" >0.9645</td>\n",
       "      <td id=\"T_bd661_row0_col2\" class=\"data row0 col2\" >0.8599</td>\n",
       "      <td id=\"T_bd661_row0_col3\" class=\"data row0 col3\" >0.8593</td>\n",
       "      <td id=\"T_bd661_row0_col4\" class=\"data row0 col4\" >0.8590</td>\n",
       "      <td id=\"T_bd661_row0_col5\" class=\"data row0 col5\" >0.7898</td>\n",
       "      <td id=\"T_bd661_row0_col6\" class=\"data row0 col6\" >0.7904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_bd661_row1_col0\" class=\"data row1 col0\" >0.8611</td>\n",
       "      <td id=\"T_bd661_row1_col1\" class=\"data row1 col1\" >0.9646</td>\n",
       "      <td id=\"T_bd661_row1_col2\" class=\"data row1 col2\" >0.8611</td>\n",
       "      <td id=\"T_bd661_row1_col3\" class=\"data row1 col3\" >0.8604</td>\n",
       "      <td id=\"T_bd661_row1_col4\" class=\"data row1 col4\" >0.8600</td>\n",
       "      <td id=\"T_bd661_row1_col5\" class=\"data row1 col5\" >0.7915</td>\n",
       "      <td id=\"T_bd661_row1_col6\" class=\"data row1 col6\" >0.7921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_bd661_row2_col0\" class=\"data row2 col0\" >0.8599</td>\n",
       "      <td id=\"T_bd661_row2_col1\" class=\"data row2 col1\" >0.9640</td>\n",
       "      <td id=\"T_bd661_row2_col2\" class=\"data row2 col2\" >0.8599</td>\n",
       "      <td id=\"T_bd661_row2_col3\" class=\"data row2 col3\" >0.8591</td>\n",
       "      <td id=\"T_bd661_row2_col4\" class=\"data row2 col4\" >0.8589</td>\n",
       "      <td id=\"T_bd661_row2_col5\" class=\"data row2 col5\" >0.7897</td>\n",
       "      <td id=\"T_bd661_row2_col6\" class=\"data row2 col6\" >0.7902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_bd661_row3_col0\" class=\"data row3 col0\" >0.8607</td>\n",
       "      <td id=\"T_bd661_row3_col1\" class=\"data row3 col1\" >0.9646</td>\n",
       "      <td id=\"T_bd661_row3_col2\" class=\"data row3 col2\" >0.8607</td>\n",
       "      <td id=\"T_bd661_row3_col3\" class=\"data row3 col3\" >0.8600</td>\n",
       "      <td id=\"T_bd661_row3_col4\" class=\"data row3 col4\" >0.8598</td>\n",
       "      <td id=\"T_bd661_row3_col5\" class=\"data row3 col5\" >0.7909</td>\n",
       "      <td id=\"T_bd661_row3_col6\" class=\"data row3 col6\" >0.7914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_bd661_row4_col0\" class=\"data row4 col0\" >0.8591</td>\n",
       "      <td id=\"T_bd661_row4_col1\" class=\"data row4 col1\" >0.9642</td>\n",
       "      <td id=\"T_bd661_row4_col2\" class=\"data row4 col2\" >0.8591</td>\n",
       "      <td id=\"T_bd661_row4_col3\" class=\"data row4 col3\" >0.8584</td>\n",
       "      <td id=\"T_bd661_row4_col4\" class=\"data row4 col4\" >0.8582</td>\n",
       "      <td id=\"T_bd661_row4_col5\" class=\"data row4 col5\" >0.7886</td>\n",
       "      <td id=\"T_bd661_row4_col6\" class=\"data row4 col6\" >0.7891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_bd661_row5_col0\" class=\"data row5 col0\" >0.8598</td>\n",
       "      <td id=\"T_bd661_row5_col1\" class=\"data row5 col1\" >0.9644</td>\n",
       "      <td id=\"T_bd661_row5_col2\" class=\"data row5 col2\" >0.8598</td>\n",
       "      <td id=\"T_bd661_row5_col3\" class=\"data row5 col3\" >0.8591</td>\n",
       "      <td id=\"T_bd661_row5_col4\" class=\"data row5 col4\" >0.8589</td>\n",
       "      <td id=\"T_bd661_row5_col5\" class=\"data row5 col5\" >0.7895</td>\n",
       "      <td id=\"T_bd661_row5_col6\" class=\"data row5 col6\" >0.7901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_bd661_row6_col0\" class=\"data row6 col0\" >0.8602</td>\n",
       "      <td id=\"T_bd661_row6_col1\" class=\"data row6 col1\" >0.9645</td>\n",
       "      <td id=\"T_bd661_row6_col2\" class=\"data row6 col2\" >0.8602</td>\n",
       "      <td id=\"T_bd661_row6_col3\" class=\"data row6 col3\" >0.8595</td>\n",
       "      <td id=\"T_bd661_row6_col4\" class=\"data row6 col4\" >0.8592</td>\n",
       "      <td id=\"T_bd661_row6_col5\" class=\"data row6 col5\" >0.7901</td>\n",
       "      <td id=\"T_bd661_row6_col6\" class=\"data row6 col6\" >0.7907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_bd661_row7_col0\" class=\"data row7 col0\" >0.8590</td>\n",
       "      <td id=\"T_bd661_row7_col1\" class=\"data row7 col1\" >0.9638</td>\n",
       "      <td id=\"T_bd661_row7_col2\" class=\"data row7 col2\" >0.8590</td>\n",
       "      <td id=\"T_bd661_row7_col3\" class=\"data row7 col3\" >0.8583</td>\n",
       "      <td id=\"T_bd661_row7_col4\" class=\"data row7 col4\" >0.8580</td>\n",
       "      <td id=\"T_bd661_row7_col5\" class=\"data row7 col5\" >0.7883</td>\n",
       "      <td id=\"T_bd661_row7_col6\" class=\"data row7 col6\" >0.7889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_bd661_row8_col0\" class=\"data row8 col0\" >0.8615</td>\n",
       "      <td id=\"T_bd661_row8_col1\" class=\"data row8 col1\" >0.9642</td>\n",
       "      <td id=\"T_bd661_row8_col2\" class=\"data row8 col2\" >0.8615</td>\n",
       "      <td id=\"T_bd661_row8_col3\" class=\"data row8 col3\" >0.8609</td>\n",
       "      <td id=\"T_bd661_row8_col4\" class=\"data row8 col4\" >0.8605</td>\n",
       "      <td id=\"T_bd661_row8_col5\" class=\"data row8 col5\" >0.7921</td>\n",
       "      <td id=\"T_bd661_row8_col6\" class=\"data row8 col6\" >0.7927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_bd661_row9_col0\" class=\"data row9 col0\" >0.8621</td>\n",
       "      <td id=\"T_bd661_row9_col1\" class=\"data row9 col1\" >0.9654</td>\n",
       "      <td id=\"T_bd661_row9_col2\" class=\"data row9 col2\" >0.8621</td>\n",
       "      <td id=\"T_bd661_row9_col3\" class=\"data row9 col3\" >0.8614</td>\n",
       "      <td id=\"T_bd661_row9_col4\" class=\"data row9 col4\" >0.8611</td>\n",
       "      <td id=\"T_bd661_row9_col5\" class=\"data row9 col5\" >0.7930</td>\n",
       "      <td id=\"T_bd661_row9_col6\" class=\"data row9 col6\" >0.7935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_bd661_row10_col0\" class=\"data row10 col0\" >0.8603</td>\n",
       "      <td id=\"T_bd661_row10_col1\" class=\"data row10 col1\" >0.9644</td>\n",
       "      <td id=\"T_bd661_row10_col2\" class=\"data row10 col2\" >0.8603</td>\n",
       "      <td id=\"T_bd661_row10_col3\" class=\"data row10 col3\" >0.8596</td>\n",
       "      <td id=\"T_bd661_row10_col4\" class=\"data row10 col4\" >0.8594</td>\n",
       "      <td id=\"T_bd661_row10_col5\" class=\"data row10 col5\" >0.7903</td>\n",
       "      <td id=\"T_bd661_row10_col6\" class=\"data row10 col6\" >0.7909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_bd661_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_bd661_row11_col0\" class=\"data row11 col0\" >0.0009</td>\n",
       "      <td id=\"T_bd661_row11_col1\" class=\"data row11 col1\" >0.0004</td>\n",
       "      <td id=\"T_bd661_row11_col2\" class=\"data row11 col2\" >0.0009</td>\n",
       "      <td id=\"T_bd661_row11_col3\" class=\"data row11 col3\" >0.0010</td>\n",
       "      <td id=\"T_bd661_row11_col4\" class=\"data row11 col4\" >0.0009</td>\n",
       "      <td id=\"T_bd661_row11_col5\" class=\"data row11 col5\" >0.0014</td>\n",
       "      <td id=\"T_bd661_row11_col6\" class=\"data row11 col6\" >0.0014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff1f06cfee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'CV-Val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CV-Val'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[39m=\u001b[39m create_model(LGBMClassifier())\n\u001b[1;32m     12\u001b[0m validation_scores \u001b[39m=\u001b[39m pull()\n\u001b[0;32m---> 13\u001b[0m accuracy_mean \u001b[39m=\u001b[39m validation_scores[\u001b[39m'\u001b[39;49m\u001b[39mAccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mCV-Val\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mMean\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[39m# save pipeline\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcombined_model\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CV-Val'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pycaret.classification import *\n",
    "from pycaret.classification import ClassificationExperiment\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "exp = ClassificationExperiment()\n",
    "dataset_path = '../../data/'\n",
    "data = pd.read_csv(f'{dataset_path}combined_dataset.csv')\n",
    "s = setup(data, target = 'signal', categorical_features=['coin'], session_id = 123, use_gpu=True)\n",
    "model = create_model(LGBMClassifier())\n",
    "validation_scores = pull()\n",
    "accuracy_mean = validation_scores['Accuracy']['CV-Val']['Mean']\n",
    "\n",
    "# save pipeline\n",
    "model_name = 'combined_model'\n",
    "save_model(model, f'../../models/{model_name}-{accuracy_mean}')\n",
    "print(f'{model_name} model saved. accuracy_mean={accuracy_mean}')\n",
    "plot_model(model, plot = 'confusion_matrix', plot_kwargs = {'percent': True})\n",
    "plot_model(model, plot = 'feature_all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import load_model, predict_model\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../../data/combined_dataset.csv')\n",
    "s = setup(data, target = 'signal', session_id = 123, use_gpu=True)\n",
    "\n",
    "# Load trained Pipeline\n",
    "model_name = 'combined_model-0.8603'\n",
    "model = load_model(f'../../models/{model_name}')\n",
    "plot_model(model, plot = 'confusion_matrix', plot_kwargs = {'percent': True})\n",
    "plot_model(model, plot = 'feature_all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
