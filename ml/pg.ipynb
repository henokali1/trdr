{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on ETHUSDT\n",
      "1654027200 1654477200 0.0% completed\n",
      "1654477200 1654927200 1.1% completed\n",
      "1654927200 1655377200 2.2% completed\n",
      "1655377200 1655827200 3.3% completed\n",
      "1655827200 1656277200 4.4% completed\n",
      "1656277200 1656727200 5.5% completed\n",
      "1656727200 1657177200 6.6% completed\n",
      "1657177200 1657627200 7.7% completed\n",
      "1657627200 1658077200 8.8% completed\n",
      "1658077200 1658527200 9.9% completed\n",
      "1658527200 1658977200 11.0% completed\n",
      "1658977200 1659427200 12.1% completed\n",
      "1659427200 1659877200 13.2% completed\n",
      "1659877200 1660327200 14.4% completed\n",
      "1660327200 1660777200 15.5% completed\n",
      "1660777200 1661227200 16.6% completed\n",
      "1661227200 1661677200 17.7% completed\n",
      "1661677200 1662127200 18.8% completed\n",
      "1662127200 1662577200 19.9% completed\n",
      "1662577200 1663027200 21.0% completed\n",
      "1663027200 1663477200 22.1% completed\n",
      "1663477200 1663927200 23.2% completed\n",
      "1663927200 1664377200 24.3% completed\n",
      "1664377200 1664827200 25.4% completed\n",
      "1664827200 1665277200 26.5% completed\n",
      "1665277200 1665727200 27.6% completed\n",
      "1665727200 1666177200 28.7% completed\n",
      "1666177200 1666627200 29.8% completed\n",
      "1666627200 1667077200 30.9% completed\n",
      "1667077200 1667527200 32.0% completed\n",
      "1667527200 1667977200 33.1% completed\n",
      "1667977200 1668427200 34.2% completed\n",
      "1668427200 1668877200 35.3% completed\n",
      "1668877200 1669327200 36.4% completed\n",
      "1669327200 1669777200 37.5% completed\n",
      "1669777200 1670227200 38.6% completed\n",
      "1670227200 1670677200 39.7% completed\n",
      "1670677200 1671127200 40.9% completed\n",
      "1671127200 1671577200 42.0% completed\n",
      "1671577200 1672027200 43.1% completed\n",
      "1672027200 1672477200 44.2% completed\n",
      "1672477200 1672927200 45.3% completed\n",
      "1672927200 1673377200 46.4% completed\n",
      "1673377200 1673827200 47.5% completed\n",
      "1673827200 1674277200 48.6% completed\n",
      "1674277200 1674727200 49.7% completed\n",
      "1674727200 1675177200 50.8% completed\n",
      "1675177200 1675627200 51.9% completed\n",
      "1675627200 1676077200 53.0% completed\n",
      "1676077200 1676527200 54.1% completed\n",
      "1676527200 1676977200 55.2% completed\n",
      "1676977200 1677427200 56.3% completed\n",
      "1677427200 1677877200 57.4% completed\n",
      "1677877200 1678327200 58.5% completed\n",
      "1678327200 1678777200 59.6% completed\n",
      "1678777200 1679227200 60.7% completed\n",
      "1679227200 1679677200 61.8% completed\n",
      "1679677200 1680127200 62.9% completed\n",
      "1680127200 1680577200 64.0% completed\n",
      "1680577200 1681027200 65.1% completed\n",
      "1681027200 1681477200 66.2% completed\n",
      "1681477200 1681927200 67.3% completed\n",
      "1681927200 1682377200 68.5% completed\n",
      "1682377200 1682827200 69.6% completed\n",
      "1682827200 1683277200 70.7% completed\n",
      "1683277200 1683727200 71.8% completed\n",
      "1683727200 1684177200 72.9% completed\n",
      "1684177200 1684627200 74.0% completed\n",
      "1684627200 1685077200 75.1% completed\n",
      "1685077200 1685527200 76.2% completed\n",
      "1685527200 1685977200 77.3% completed\n",
      "1685977200 1686427200 78.4% completed\n",
      "1686427200 1686877200 79.5% completed\n",
      "1686877200 1687327200 80.6% completed\n",
      "1687327200 1687777200 81.7% completed\n",
      "1687777200 1688227200 82.8% completed\n",
      "1688227200 1688677200 83.9% completed\n",
      "1688677200 1689127200 85.0% completed\n",
      "1689127200 1689577200 86.1% completed\n",
      "1689577200 1690027200 87.2% completed\n",
      "1690027200 1690477200 88.3% completed\n",
      "1690477200 1690927200 89.4% completed\n",
      "1690927200 1691377200 90.5% completed\n",
      "1691377200 1691827200 91.6% completed\n",
      "1691827200 1692277200 92.7% completed\n",
      "1692277200 1692727200 93.8% completed\n",
      "1692727200 1693177200 95.0% completed\n",
      "1693177200 1693627200 96.1% completed\n",
      "1693627200 1694077200 97.2% completed\n",
      "1694077200 1694527200 98.3% completed\n",
      "1694527200 1694785095 100% completed\n",
      "Historical Data of ETHUSDT Downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X] Please install TA-Lib to use 2crows. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use 3blackcrows. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use 3inside. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use 3linestrike. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use 3outside. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use 3starsinsouth. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use 3whitesoldiers. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use abandonedbaby. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use advanceblock. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use belthold. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use breakaway. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use closingmarubozu. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use concealbabyswall. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use counterattack. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use darkcloudcover. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use dojistar. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use dragonflydoji. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use engulfing. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use eveningdojistar. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use eveningstar. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use gapsidesidewhite. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use gravestonedoji. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use hammer. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use hangingman. (pip install TA-Lib)\n",
      "[!] VWAP volume series is not datetime ordered. Results may not be as expected.\n",
      "[X] Please install TA-Lib to use harami. (pip install TA-Lib)[!] VWAP price series is not datetime ordered. Results may not be as expected.\n",
      "\n",
      "[X] Please install TA-Lib to use haramicross. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use highwave. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use hikkake. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use hikkakemod. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use homingpigeon. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use identical3crows. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use inneck. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use invertedhammer. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use kicking. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use kickingbylength. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use ladderbottom. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use longleggeddoji. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use longline. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use marubozu. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use matchinglow. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use mathold. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use morningdojistar. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use morningstar. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use onneck. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use piercing. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use rickshawman. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use risefall3methods. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use separatinglines. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use shootingstar. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use shortline. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use spinningtop. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use stalledpattern. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use sticksandwich. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use takuri. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use tasukigap. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use thrusting. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use tristar. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use unique3river. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use upsidegap2crows. (pip install TA-Lib)\n",
      "[X] Please install TA-Lib to use xsidegap3methods. (pip install TA-Lib)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "123it [00:10, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'RangeIndex' object has no attribute 'to_period'\n",
      "ETHUSDT Features Generated and saved\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "from binance.client import Client\n",
    "from time import time\n",
    "import pickle as pickle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from pycaret.classification import load_model, predict_model\n",
    "from pycaret.classification import *\n",
    "from pycaret.classification import ClassificationExperiment\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def get_client():\n",
    "    fn = '../../key/binance-key.pickle'\n",
    "    # fn = '/home/era/key/binance-key.pickle'\n",
    "    with open(fn, 'rb') as handle:\n",
    "        k = pickle.load(handle)\n",
    "    return Client(k['API_KEY'], k['API_SECRET'])\n",
    "\n",
    "\n",
    "client = get_client()\n",
    "\n",
    "\n",
    "def get_unix_timestamp(date_string):\n",
    "    \"\"\"\n",
    "    Converts the input date string to Unix timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        date_string (str): Input date string in the format \"dd/mm/yyyy hh:mm:ss\".\n",
    "\n",
    "    Returns:\n",
    "        int: Unix timestamp of the given date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_string, \"%d/%m/%Y %H:%M:%S\")\n",
    "        timestamp = int(date_obj.timestamp())\n",
    "        return timestamp\n",
    "    except ValueError:\n",
    "        print(\"Invalid date format. Please use the format 'dd/mm/yyyy hh:mm:ss'.\")\n",
    "        return None\n",
    "\n",
    "def get_historical_data(start_timestamp, end_timestamp, coin_pair): \n",
    "    data = []\n",
    "    tot = (end_timestamp - start_timestamp)/(900*500)\n",
    "    cntr = 0\n",
    "    for current_sts in range(start_timestamp, end_timestamp+1, 900*500):\n",
    "        next_ets = current_sts + 900*500 if (current_sts + 900*500) < end_timestamp else end_timestamp\n",
    "        print(current_sts, next_ets, f'100% completed') if next_ets == end_timestamp else print(current_sts, next_ets, f'{round(cntr*100/tot, 1)}% completed')\n",
    "        cntr += 1\n",
    "        # Futures market\n",
    "        klines = client.futures_historical_klines(coin_pair, '15m', current_sts*1000, next_ets*1000, limit=500)\n",
    "        # Spot market\n",
    "        # klines = client.get_historical_klines(coin_pair, interval, current_sts*1000, next_ets*1000, limit=500)\n",
    "        \n",
    "        for kline in klines:\n",
    "            timestamp = kline[0]/1000\n",
    "            open_price = float(kline[1])\n",
    "            high_price = float(kline[2])\n",
    "            low_price = float(kline[3])\n",
    "            close_price = float(kline[4])\n",
    "            volume = float(kline[5])\n",
    "\n",
    "            data.append([timestamp, open_price, high_price, low_price, close_price, volume])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    # df.to_csv(f'/home/ubuntu/data/{coin_pair}-{interval}.csv', index=False)\n",
    "    # print('Data Exported')\n",
    "    print(f'Historical Data of {coin_pair} Downloaded')\n",
    "    return df\n",
    "\n",
    "def generate_features(start_timestamp, end_timestamp, coin_pair):\n",
    "    df = get_historical_data(start_timestamp, end_timestamp, coin_pair)\n",
    "    candlestick_frame = 12\n",
    "    pnl_threshold = 3\n",
    "\n",
    "\n",
    "    # df = pd.read_csv('/home/ubuntu/data/ETHUSDT-15m.csv')\n",
    "    try:\n",
    "        df.ta.strategy(\"all\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ts = list(df['time'])\n",
    "        open = list(df['open'])\n",
    "        high = list(df['high'])\n",
    "        low = list(df['low'])\n",
    "        close = list(df['close'])\n",
    "        volume = list(df['volume'])\n",
    "        tot = len(ts)\n",
    "        long_runup_lst = []\n",
    "        long_drawdown_lst = []\n",
    "        short_runup_lst = []\n",
    "        short_drawdown_lst = []\n",
    "\n",
    "        for idx in range(tot):\n",
    "            if (idx >= candlestick_frame) and (idx <= tot - candlestick_frame):\n",
    "                max_high = max(high[idx+1:idx+candlestick_frame])\n",
    "                min_low = min(low[idx+1:idx+candlestick_frame])\n",
    "                entry_price = open[idx+1]\n",
    "                long_runup_lst.append(round((max_high*100/entry_price)-100, 6))\n",
    "                long_drawdown_lst.append(round((min_low*100/entry_price)-100, 6))\n",
    "                short_runup_lst.append(round((entry_price*100/min_low)-100, 6))\n",
    "                short_drawdown_lst.append(round((entry_price*100/max_high)-100, 6))\n",
    "            else:\n",
    "                long_runup_lst.append(0)\n",
    "                long_drawdown_lst.append(0)\n",
    "                short_runup_lst.append(0)\n",
    "                short_drawdown_lst.append(0)     \n",
    "\n",
    "\n",
    "        long=[]\n",
    "        short=[]\n",
    "        dont_trade=[]\n",
    "        signal = []\n",
    "\n",
    "        for idx in range(tot):\n",
    "            if (idx >= candlestick_frame) and (idx <= tot - candlestick_frame):\n",
    "                if long_runup_lst[idx] >= pnl_threshold:\n",
    "                    signal.append('long')\n",
    "                elif short_runup_lst[idx] >= pnl_threshold:\n",
    "                    signal.append('short')\n",
    "                else:\n",
    "                    signal.append('dont_trade')\n",
    "            else:\n",
    "                signal.append('dont_trade')\n",
    "\n",
    "        df['coin'] = [coin_pair]*len(signal)\n",
    "        df['signal'] = signal\n",
    "\n",
    "        long_indices = df[df['signal'].str.contains('long', case=False)].index\n",
    "        short_indices = df[df['signal'].str.contains('short', case=False)].index\n",
    "        dont_trade_indices = list(df[df['signal'].str.contains('dont_trade', case=False)].index)\n",
    "        num_indices_to_pick  = len(dont_trade_indices) - min([len(long_indices), len(short_indices)])\n",
    "        random_indices = random.sample(dont_trade_indices, num_indices_to_pick)\n",
    "        df = df.drop(random_indices)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        df = df.drop(columns = ['time', 'open', 'high', 'low', 'close', 'volume'], axis=1)\n",
    "        dataset_fn = f'../../data/{coin_pair}-dataset.csv'\n",
    "        pd.DataFrame(df).to_csv(dataset_fn, index=False)\n",
    "        print(f'{coin_pair} Features Generated and saved')\n",
    "\n",
    "\n",
    "# coins_list = ['BTC', 'ETH', 'BNB', 'XRP', 'ADA', 'DOGE', 'SOL', 'TRX', 'DOT', 'MATIC', 'LTC', 'BCH', 'AVAX', 'XLM', 'LINK', 'UNI', 'XMR', 'ATOM', 'ETC', 'HBAR', 'ICP', 'FIL', 'LDO', 'APT', 'ARB', 'QNT', 'VET', 'NEAR', 'OP', 'MKR', 'GRT', 'AAVE', 'ALGO', 'AXS', 'EGLD', 'STX', 'SAND', 'XTZ', 'EOS', 'INJ', 'THETA', 'IMX', 'SNX', 'MANA', 'FTM', 'RUNE', 'APE', 'RNDR', 'NEO', 'KAVA', 'FLOW', 'CHZ', 'GALA', 'KLAY', 'SUI', 'FXS', 'ZEC', 'CFX', 'CRV', 'MINA', 'COMP', 'GMX', 'DYDX', 'WOO', 'ASTR']\n",
    "coins_list = ['ETH']\n",
    "\n",
    "for i in range(len(coins_list)):\n",
    "    coin_pair = f'{coins_list[i]}USDT'\n",
    "    print('working on', coin_pair)\n",
    "    # start_timestamp = get_unix_timestamp('1/1/2016 00:00:00')\n",
    "    start_timestamp = get_unix_timestamp('1/6/2022 00:00:00')\n",
    "    end_timestamp = int(time())\n",
    "    generate_features(start_timestamp, end_timestamp, coin_pair)\n",
    "    \n",
    "coin_feaures = [f'{val}USDT-dataset.csv' for val in coins_list]\n",
    "dfs=[]\n",
    "dataset_path = '../../data/'\n",
    "\n",
    "for fn in coin_feaures:\n",
    "    dfs.append(pd.read_csv(f'{dataset_path}{fn}'))\n",
    "\n",
    "concatenated_df = pd.concat(dfs, ignore_index=True)  # Set ignore_index=True to reset index\n",
    "pd.DataFrame(concatenated_df).to_csv(f'{dataset_path}combined_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column must not contain NaN values: value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mindex\n\u001b[1;32m      7\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mmelt(id_vars\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m, var_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msort_values([\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m X \u001b[39m=\u001b[39m extract_features(df[df[\u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m<\u001b[39;49m \u001b[39m500\u001b[39;49m], column_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m, column_sort\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtime\u001b[39;49m\u001b[39m\"\u001b[39;49m, impute_function\u001b[39m=\u001b[39;49mimpute)\n\u001b[1;32m     11\u001b[0m \u001b[39m# exp = ClassificationExperiment()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# dataset_path = '../../data/'\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# data = pd.read_csv(f'{dataset_path}combined_dataset.csv')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m# plot_model(model, plot = 'confusion_matrix', plot_kwargs = {'percent': True})\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# plot_model(model, plot = 'feature_all')\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tsfresh/feature_extraction/extraction.py:164\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(timeseries_container, default_fc_parameters, kind_to_fc_parameters, column_id, column_sort, column_kind, column_value, chunksize, n_jobs, show_warnings, disable_progressbar, impute_function, profile, profiling_filename, profiling_sorting, distributor, pivot)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 164\u001b[0m result \u001b[39m=\u001b[39m _do_extraction(\n\u001b[1;32m    165\u001b[0m     df\u001b[39m=\u001b[39;49mtimeseries_container,\n\u001b[1;32m    166\u001b[0m     column_id\u001b[39m=\u001b[39;49mcolumn_id,\n\u001b[1;32m    167\u001b[0m     column_value\u001b[39m=\u001b[39;49mcolumn_value,\n\u001b[1;32m    168\u001b[0m     column_kind\u001b[39m=\u001b[39;49mcolumn_kind,\n\u001b[1;32m    169\u001b[0m     column_sort\u001b[39m=\u001b[39;49mcolumn_sort,\n\u001b[1;32m    170\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    171\u001b[0m     chunk_size\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m    172\u001b[0m     disable_progressbar\u001b[39m=\u001b[39;49mdisable_progressbar,\n\u001b[1;32m    173\u001b[0m     show_warnings\u001b[39m=\u001b[39;49mshow_warnings,\n\u001b[1;32m    174\u001b[0m     default_fc_parameters\u001b[39m=\u001b[39;49mdefault_fc_parameters,\n\u001b[1;32m    175\u001b[0m     kind_to_fc_parameters\u001b[39m=\u001b[39;49mkind_to_fc_parameters,\n\u001b[1;32m    176\u001b[0m     distributor\u001b[39m=\u001b[39;49mdistributor,\n\u001b[1;32m    177\u001b[0m     pivot\u001b[39m=\u001b[39;49mpivot,\n\u001b[1;32m    178\u001b[0m )\n\u001b[1;32m    180\u001b[0m \u001b[39m# Impute the result if requested\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m impute_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tsfresh/feature_extraction/extraction.py:260\u001b[0m, in \u001b[0;36m_do_extraction\u001b[0;34m(df, column_id, column_value, column_kind, column_sort, default_fc_parameters, kind_to_fc_parameters, n_jobs, chunk_size, disable_progressbar, show_warnings, distributor, pivot)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_extraction\u001b[39m(\n\u001b[1;32m    194\u001b[0m     df,\n\u001b[1;32m    195\u001b[0m     column_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m     pivot,\n\u001b[1;32m    207\u001b[0m ):\n\u001b[1;32m    208\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m    Wrapper around the _do_extraction_on_chunk, which calls it on all chunks in the data frame.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m    A chunk is a subset of the data, with a given kind and id - so a single time series.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39m    :rtype: pd.DataFrame\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     data \u001b[39m=\u001b[39m to_tsdata(df, column_id, column_kind, column_value, column_sort)\n\u001b[1;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m distributor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    263\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Iterable):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tsfresh/feature_extraction/data.py:478\u001b[0m, in \u001b[0;36mto_tsdata\u001b[0;34m(df, column_id, column_kind, column_value, column_sort)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[39mreturn\u001b[39;00m WideTsFrameAdapter(df, column_id, column_sort, [column_value])\n\u001b[1;32m    477\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m             \u001b[39mreturn\u001b[39;00m WideTsFrameAdapter(df, column_id, column_sort)\n\u001b[1;32m    480\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(df, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    481\u001b[0m     \u001b[39mreturn\u001b[39;00m TsDictAdapter(df, column_id, column_value, column_sort)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tsfresh/feature_extraction/data.py:186\u001b[0m, in \u001b[0;36mWideTsFrameAdapter.__init__\u001b[0;34m(self, df, column_id, column_sort, value_columns)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m value_columns:\n\u001b[1;32m    184\u001b[0m     value_columns \u001b[39m=\u001b[39m _get_value_columns(df, column_id, column_sort)\n\u001b[0;32m--> 186\u001b[0m _check_nan(df, \u001b[39m*\u001b[39;49mvalue_columns)\n\u001b[1;32m    187\u001b[0m _check_colname(\u001b[39m*\u001b[39mvalue_columns)\n\u001b[1;32m    189\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_columns \u001b[39m=\u001b[39m value_columns\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tsfresh/feature_extraction/data.py:145\u001b[0m, in \u001b[0;36m_check_nan\u001b[0;34m(df, *columns)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mColumn not found: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(col))\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m df[col]\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39many():\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mColumn must not contain NaN values: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(col))\n",
      "\u001b[0;31mValueError\u001b[0m: Column must not contain NaN values: value"
     ]
    }
   ],
   "source": [
    "from tsfresh import extract_features, extract_relevant_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "\n",
    "df = concatenated_df\n",
    "df[\"id\"] = df.index\n",
    "df = df.melt(id_vars=\"id\", var_name=\"time\").sort_values([\"id\", \"time\"]).reset_index(drop=True)\n",
    "X = extract_features(df[df[\"id\"] < 500], column_id=\"id\", column_sort=\"time\", impute_function=impute)\n",
    "\n",
    "\n",
    "# exp = ClassificationExperiment()\n",
    "# dataset_path = '../../data/'\n",
    "# data = pd.read_csv(f'{dataset_path}combined_dataset.csv')\n",
    "# data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# s = setup(data, target = 'signal', categorical_features=['coin'], session_id = 123, use_gpu=True)\n",
    "# model = create_model(LGBMClassifier())\n",
    "# validation_scores = pull()\n",
    "# accuracy_mean = validation_scores['Accuracy']['Mean']\n",
    "\n",
    "# # save pipeline\n",
    "# model_name = 'combined_model_tsfresh'\n",
    "# save_model(model, f'../../models/{model_name}-{accuracy_mean}')\n",
    "# print(f'{model_name} model saved. accuracy_mean={accuracy_mean}')\n",
    "# plot_model(model, plot = 'confusion_matrix', plot_kwargs = {'percent': True})\n",
    "# plot_model(model, plot = 'feature_all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ABER_ATR_5_15</td>\n",
       "      <td>8.919736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ABER_SG_5_15</td>\n",
       "      <td>1959.740403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ABER_XG_5_15</td>\n",
       "      <td>1941.900931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ABER_ZG_5_15</td>\n",
       "      <td>1950.820667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ACCBL_20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164715</th>\n",
       "      <td>5519</td>\n",
       "      <td>coin</td>\n",
       "      <td>ETHUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164716</th>\n",
       "      <td>5519</td>\n",
       "      <td>high_Z_30_1</td>\n",
       "      <td>1.531344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164717</th>\n",
       "      <td>5519</td>\n",
       "      <td>low_Z_30_1</td>\n",
       "      <td>0.119526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164718</th>\n",
       "      <td>5519</td>\n",
       "      <td>open_Z_30_1</td>\n",
       "      <td>-0.436634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164719</th>\n",
       "      <td>5519</td>\n",
       "      <td>signal</td>\n",
       "      <td>dont_trade</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164720 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id           time        value\n",
       "0           0  ABER_ATR_5_15     8.919736\n",
       "1           0   ABER_SG_5_15  1959.740403\n",
       "2           0   ABER_XG_5_15  1941.900931\n",
       "3           0   ABER_ZG_5_15  1950.820667\n",
       "4           0       ACCBL_20          NaN\n",
       "...       ...            ...          ...\n",
       "1164715  5519           coin      ETHUSDT\n",
       "1164716  5519    high_Z_30_1     1.531344\n",
       "1164717  5519     low_Z_30_1     0.119526\n",
       "1164718  5519    open_Z_30_1    -0.436634\n",
       "1164719  5519         signal   dont_trade\n",
       "\n",
       "[1164720 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import load_model, predict_model\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../../data/combined_dataset.csv')\n",
    "s = setup(data, target = 'signal', session_id = 123, use_gpu=True)\n",
    "\n",
    "# Load trained Pipeline\n",
    "model_name = 'combined_model_tsfresh'\n",
    "model = load_model(f'../../models/{model_name}')\n",
    "plot_model(model, plot = 'confusion_matrix', plot_kwargs = {'percent': True})\n",
    "# plot_model(model, plot = 'feature_all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
