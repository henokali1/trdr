{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "downloads_path = str(Path.home() / \"Downloads\")\n",
    "documents_path = str(Path.home() / \"Documents\")\n",
    "hd_dl_fn = f'{downloads_path}\\\\hd_dl.csv'\n",
    "qualifying_trades_fn = f'{downloads_path}\\\\qualifying_trades.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "def read_local_hd(fn):\n",
    "    return pd.read_csv(fn)\n",
    "\n",
    "def save_dict_to_pickle(dictionary, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(dictionary, file)\n",
    "        print(f'Saved {filename}')\n",
    "\n",
    "def get_chunks(df):\n",
    "    lst = list(df['close_pc'])\n",
    "    r=[]\n",
    "    tot = len(lst)\n",
    "    for idx, val in enumerate(lst):\n",
    "        if idx < chunk_size:\n",
    "            r.append([0]*chunk_size)\n",
    "        else:\n",
    "            r.append(lst[idx-chunk_size:idx])\n",
    "    df['raw_chunks'] = r\n",
    "    # df.to_csv(f'{downloads_path}/raw_chunks.csv', index=False)\n",
    "    # print('Raw Chunks Exported!', f'{downloads_path}/raw_chunks.csv')\n",
    "    return df\n",
    "\n",
    "def get_pc(hd_df):\n",
    "    close_price = list(hd_df['close'])\n",
    "    ts = list(hd_df['time'])\n",
    "    pc = []\n",
    "    for idx, val in enumerate(close_price):\n",
    "        if idx==0:\n",
    "            pc.append({'time': ts[0], 'pc': 0})\n",
    "        else:\n",
    "            previous_price = close_price[idx-1]\n",
    "            current_price = val\n",
    "            pc_val = (round((current_price*100/previous_price)-100, 4))\n",
    "            pc.append({'time': ts[idx], 'pc': pc_val})\n",
    "    pc_df = pd.DataFrame(pc)\n",
    "    return pc_df\n",
    "\n",
    "def get_trades(hd_df, chunk_size):\n",
    "    close_price = list(hd_df['close'])\n",
    "    high_price = list(hd_df['high'])\n",
    "    low_price = list(hd_df['low'])\n",
    "    longs=[]\n",
    "    shorts=[]\n",
    "    long_qualified = []\n",
    "    short_qualified = []\n",
    "    tot = len(close_price)\n",
    "    for idx in range(len(close_price)):\n",
    "        if (idx + chunk_size) <= tot:\n",
    "            cp = close_price[idx]\n",
    "            max_price = max(high_price[idx:idx + chunk_size])\n",
    "            min_price = min(low_price[idx:idx + chunk_size])\n",
    "            long_roi = round((max_price*100/cp)-100, 4)\n",
    "            short_roi = round((cp*100/min_price)-100, 4)\n",
    "            longs.append({'long': long_roi})\n",
    "            long_qualified.append(long_roi >= roi_threshold)\n",
    "            shorts.append({'short': short_roi})\n",
    "            short_qualified.append(short_roi >= roi_threshold)\n",
    "        else:\n",
    "            long_roi = 0.0\n",
    "            short_roi = 0.0\n",
    "            longs.append({'long': long_roi})\n",
    "            long_qualified.append(False)\n",
    "            shorts.append({'short': short_roi})\n",
    "            short_qualified.append(False)\n",
    "    long_trades_df = pd.DataFrame(longs)\n",
    "    short_trades_df = pd.DataFrame(shorts)\n",
    "    long_qualified_df = pd.DataFrame(long_qualified)\n",
    "    short_qualified_df = pd.DataFrame(short_qualified)\n",
    "    return long_trades_df, short_trades_df, long_qualified, short_qualified\n",
    "\n",
    "def get_qualifying_trades(df):\n",
    "    close_pc_df = get_pc(df)\n",
    "    long_trades_df, short_trades_df, long_qualified, short_qualified = get_trades(df, chunk_size)\n",
    "    df['close_pc'] = close_pc_df['pc']\n",
    "    df['long'] = long_trades_df\n",
    "    df['short'] = short_trades_df\n",
    "    df['long_qualified'] = long_qualified\n",
    "    df['short_qualified'] = short_qualified\n",
    "    get_chunks(df)\n",
    "    filtered_df = df[(df['long_qualified']) | (df['short_qualified'])]\n",
    "    # filtered_df = df[(df['long_qualified'])]\n",
    "    # filtered_df.to_csv(qualifying_trades_fn, index=False)\n",
    "    # print('Qualifying Trades Data Exported!', qualifying_trades_fn)\n",
    "    return filtered_df\n",
    "\n",
    "def get_ref_chunks():\n",
    "    hd = read_local_hd(hd_dl_fn)\n",
    "    qualifying_trades = get_qualifying_trades(hd)\n",
    "    ref_chunks_df = pd.DataFrame()\n",
    "    ts = list(qualifying_trades['time'])\n",
    "    ref_chunks_lst = list(qualifying_trades['raw_chunks'])\n",
    "    long_qualified = list(qualifying_trades['long_qualified'])\n",
    "    short_qualified = list(qualifying_trades['short_qualified'])\n",
    "    ref_chunks_df['ts'] = ts\n",
    "    ref_chunks_df['ref_chunks'] = ref_chunks_lst\n",
    "    ref_chunks_df['long_qualified'] = long_qualified\n",
    "    ref_chunks_df['short_qualified'] = short_qualified\n",
    "    ref_chunks_df.to_csv(f'{downloads_path}\\\\ref_chunks.csv', index=False)\n",
    "    print(f'Referance Chunks  Exported! {downloads_path}\\\\ref_chunks.csv')\n",
    "    return ref_chunks_df\n",
    "\n",
    "def get_tst_chunks():\n",
    "    hd = read_local_hd(hd_dl_fn)\n",
    "    close_pc_df = get_pc(hd)\n",
    "    hd['close_pc'] = list(close_pc_df['pc'])\n",
    "    chunks_df = get_chunks(hd)\n",
    "    tst_chunks_df = pd.DataFrame()\n",
    "    tst_chunks_df['ts'] = list(chunks_df['time'])\n",
    "    tst_chunks_df['tst_chunks'] = list(chunks_df['raw_chunks'])\n",
    "    tst_chunks_df.to_csv(f'{downloads_path}\\\\tst_chunks.csv', index=False)\n",
    "    print(f'tst Chunks Exported! {downloads_path}\\\\tst_chunks.csv')\n",
    "    return tst_chunks_df\n",
    "\n",
    "def format_elapsed_time(seconds):\n",
    "    days = seconds // (24 * 3600)\n",
    "    seconds %= 24 * 3600\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return f\"{days:03}d:{hours:02}h:{minutes:02}m:{seconds:02}s\"\n",
    "\n",
    "def all_zeros(lst):\n",
    "    return all(x == 0 for x in lst)\n",
    "\n",
    "def get_percent_dissimilarity(ref_lst_chunk, tst_lst_chunk):\n",
    "    percent_dissimilarity = []\n",
    "    for idx in range(len(ref_lst_chunk)):\n",
    "        if ref_lst_chunk[idx] != 0:\n",
    "            percent_dissimilarity.append(abs(round((tst_lst_chunk[idx]*100/ref_lst_chunk[idx])-100, 4)))\n",
    "        else:\n",
    "            percent_dissimilarity.append(9999)\n",
    "    avg = round(sum(percent_dissimilarity)/len(percent_dissimilarity), 4)\n",
    "    return avg\n",
    "\n",
    "def measure_dissimilarity(ref_chunks_lst_dict, tst_chunks_lst_dict):\n",
    "    r= []\n",
    "    qualified_tst_ts_lst = []\n",
    "    qualified_avg_dissimilarity_lst = []\n",
    "    start_ts = time()\n",
    "    tot = len(ref_chunks_lst_dict)\n",
    "    for ref_idx, ref_val in enumerate(ref_chunks_lst_dict):\n",
    "        # if(ref_idx%10 == 0) and ref_idx != 0:\n",
    "        if ref_idx != 0:\n",
    "            completed = round(ref_idx*100/tot, 1)\n",
    "            remaining = round(100-completed)\n",
    "            cur_ts = time()\n",
    "            ts_diff = int(cur_ts - start_ts)\n",
    "            estimated_tm_to_complete = round(int(ts_diff*(tot-ref_idx)/ref_idx), 1)\n",
    "            print(f\"\\rElapsed Time: {format_elapsed_time(ts_diff)} \\t Completed: {completed}% \\tRemaining: {remaining}% \\tETA: {format_elapsed_time(estimated_tm_to_complete)}                               \", end=\"\")\n",
    "        for tst_idx, tst_val in enumerate(tst_chunks_lst_dict):\n",
    "            if ((all_zeros(ref_val['ref_chunks'])) or (all_zeros(tst_val['tst_chunks'])) or (tst_val['tst_chunks'] == ref_val['ref_chunks'])):\n",
    "                continue\n",
    "            else:\n",
    "                avg_dissimilarity = get_percent_dissimilarity(ref_val['ref_chunks'], tst_val['tst_chunks'])\n",
    "                if avg_dissimilarity <= dissimilarity_threshold:\n",
    "                    qualified_tst_ts_lst.append(tst_val['ts'])\n",
    "                    qualified_avg_dissimilarity_lst.append(avg_dissimilarity)\n",
    "\n",
    "        pattern_occurrence = len(qualified_avg_dissimilarity_lst)\n",
    "        if pattern_occurrence >= min_pattern_occurrence:\n",
    "            avg_avg_dissimilarity = round(sum(qualified_avg_dissimilarity_lst)/pattern_occurrence, 4)\n",
    "            r.append({\n",
    "                'avg_avg_dissimilarity': avg_avg_dissimilarity,\n",
    "                'pattern_occurrence': pattern_occurrence,\n",
    "                'ref_ts': ref_val['ts'],\n",
    "                'qualified_tst_ts_lst': qualified_tst_ts_lst,\n",
    "                'qualified_avg_dissimilarity_lst': qualified_avg_dissimilarity_lst,\n",
    "            })\n",
    "            save_dict_to_pickle(r, f'{downloads_path}\\\\r.pkl')\n",
    "        qualified_tst_ts_lst = []\n",
    "        qualified_avg_dissimilarity_lst = []\n",
    "    print('\\nExporting dissimilarity.csv')\n",
    "    dissimilarity_df = pd.DataFrame(r)\n",
    "    dissimilarity_df.to_csv(f'{downloads_path}\\\\dissimilarity.csv', index=False)\n",
    "    print('dissimilarity.csv Exported!')\n",
    "    return dissimilarity_df\n",
    "\n",
    "config = get_config('config.json')\n",
    "candlestick = config['candlestick']\n",
    "chunk_size = config['chunk_size']\n",
    "roi_threshold = config['roi_threshold']\n",
    "sm_threshold = config['sm_threshold']\n",
    "min_pattern_occurrence = config['min_pattern_occurrence']\n",
    "dissimilarity_threshold = config['dissimilarity_threshold']\n",
    "\n",
    "ref_chunks_df = get_ref_chunks()\n",
    "ref_chunks_lst_dict = ref_chunks_df.to_dict(orient='records')\n",
    "tst_chunks_df = get_tst_chunks()\n",
    "tst_chunks_lst_dict = tst_chunks_df.to_dict(orient='records')\n",
    "\n",
    "dissimilarity = measure_dissimilarity(ref_chunks_lst_dict, tst_chunks_lst_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
