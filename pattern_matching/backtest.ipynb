{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "from binance.enums import *\n",
    "from time import time\n",
    "import pickle as pickle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import math\n",
    "import ast\n",
    "from time import time\n",
    "import pickle\n",
    "\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "downloads_path = str(Path.home() / \"Downloads\")\n",
    "documents_path = str(Path.home() / \"Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\HK\\Downloads/sorted_by_pattern_occurrence.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extract good long patterns\n",
    "def load_dict_from_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        dictionary = pickle.load(file)\n",
    "    print(f'Reading {filename}')\n",
    "    return dictionary\n",
    "\n",
    "long_sorted_by_pattern_occurrence = load_dict_from_pickle(f'{downloads_path}/sorted_by_pattern_occurrence.pkl')\n",
    "long_patterns = []\n",
    "for val in long_sorted_by_pattern_occurrence:\n",
    "    pattern_occurrence = val['pattern_occurrence']\n",
    "    if pattern_occurrence > 500:\n",
    "        long_patterns.append(val['tst_chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719721304 1720171304 0.0% completed\n",
      "1720171304 1720621304 19.1% completed\n",
      "1720621304 1721071304 38.1% completed\n",
      "1721071304 1721521304 57.2% completed\n",
      "1721521304 1721971304 76.3% completed\n",
      "1721971304 1722081600 100% completed\n",
      "Historical Data Exported!\t C:\\Users\\HK\\Downloads/unseen_hd_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# Download unseen historical data\n",
    "\n",
    "def get_client():\n",
    "    fn = f'{documents_path}\\\\key\\\\binance-key.pickle'\n",
    "    with open(fn, 'rb') as handle:\n",
    "        k = pickle.load(handle)\n",
    "    return Client(k['API_KEY'], k['API_SECRET'])\n",
    "\n",
    "def get_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "def download_hd(start_timestamp, end_timestamp, candlestick): \n",
    "    client = get_client()\n",
    "    data = []\n",
    "    tot = (end_timestamp - start_timestamp)/(900*500)\n",
    "    cntr = 0\n",
    "    for current_sts in range(start_timestamp, end_timestamp+1, 900*500):\n",
    "        next_ets = current_sts + 900*500 if (current_sts + 900*500) < end_timestamp else end_timestamp\n",
    "        print(current_sts, next_ets, f'100% completed') if next_ets == end_timestamp else print(current_sts, next_ets, f'{round(cntr*100/tot, 1)}% completed')\n",
    "        cntr += 1\n",
    "        \n",
    "        klines = client.futures_historical_klines('BTCUSDT', candlestick, current_sts*1000, next_ets*1000, limit=500)\n",
    "        \n",
    "        for kline in klines:\n",
    "            timestamp = kline[0]/1000\n",
    "            open_price = float(kline[1])\n",
    "            high_price = float(kline[2])\n",
    "            low_price = float(kline[3])\n",
    "            close_price = float(kline[4])\n",
    "            volume = float(kline[5])\n",
    "\n",
    "            data.append([timestamp, open_price, high_price, low_price, close_price, volume])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    hd_dl_fn = f'{downloads_path}/unseen_hd_raw.csv'\n",
    "    df.to_csv(hd_dl_fn, index=False)\n",
    "    print('Historical Data Exported!\\t', hd_dl_fn)\n",
    "    return df\n",
    "\n",
    "\n",
    "config = get_config('config.json')\n",
    "candlestick = config['candlestick']\n",
    "chunk_size = config['chunk_size']\n",
    "roi_threshold = config['roi_threshold']\n",
    "sm_threshold = config['sm_threshold']\n",
    "\n",
    "start_timestamp = 1719721304 #  Sunday, June 30, 2024 8:21:44 AM GMT+04:00\n",
    "end_timestamp = 1722081600 #  July 27, 2024 4:00:00 PM GMT+04:00\n",
    "\n",
    "unseen_hd = download_hd(start_timestamp, end_timestamp, candlestick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backtest_chunks Chunks Exported! C:\\Users\\HK\\Downloads/backtest_chunks.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_pc(hd_df):\n",
    "    close_price = list(hd_df['close'])\n",
    "    ts = list(hd_df['time'])\n",
    "    pc = []\n",
    "    for idx, val in enumerate(close_price):\n",
    "        if idx==0:\n",
    "            pc.append({'time': ts[0], 'close_pc': 0})\n",
    "        else:\n",
    "            previous_price = close_price[idx-1]\n",
    "            current_price = val\n",
    "            pc_val = (round((current_price*100/previous_price)-100, 4))\n",
    "            pc.append({'time': ts[idx], 'close_pc': pc_val})\n",
    "    pc_df = pd.DataFrame(pc)\n",
    "    return pc_df\n",
    "\n",
    "def get_chunks(qt_df):\n",
    "    lst = list(qt_df['close_pc'])\n",
    "    r=[]\n",
    "    tot = len(lst)\n",
    "    for idx, val in enumerate(lst):\n",
    "        if idx < chunk_size:\n",
    "            r.append([0]*chunk_size)\n",
    "        else:\n",
    "            r.append(lst[idx-chunk_size:idx])\n",
    "    qt_df['ref_chunks'] = r\n",
    "    qt_df.to_csv(f'{downloads_path}/backtest_chunks.csv', index=False)\n",
    "    print('backtest_chunks Chunks Exported!', f'{downloads_path}/backtest_chunks.csv')\n",
    "    return qt_df\n",
    "\n",
    "hd_dl_fn = f'{downloads_path}/unseen_hd_raw.csv'\n",
    "unseen_hd = pd.read_csv(hd_dl_fn)\n",
    "pc_df = get_pc(unseen_hd)\n",
    "backtest_chunks = get_chunks(pc_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 000d:00h:00m:35s \t Completed: 99.9% \tRemaining: 0% \tETA: 000d:00h:00m:00ss"
     ]
    }
   ],
   "source": [
    "\n",
    "def all_zeros(lst):\n",
    "    return all(x == 0 for x in lst)\n",
    "\n",
    "def format_elapsed_time(seconds):\n",
    "    days = seconds // (24 * 3600)\n",
    "    seconds %= 24 * 3600\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "\n",
    "    return f\"{days:03}d:{hours:02}h:{minutes:02}m:{seconds:02}s\"\n",
    "\n",
    "def chunk_diff(long_db_chunk, tst_lst, tst_df):\n",
    "    ts = list(tst_df['time'])\n",
    "    test_chunks = list(tst_df['ref_chunks'])\n",
    "    cntr = 0\n",
    "    chunk_size = len(tst_lst[0])\n",
    "    r = []\n",
    "    tot = len(tst_lst)\n",
    "    start_ts = int(time())\n",
    "    log_msg = []\n",
    "    enter_long_pos = []\n",
    "    for tst_lst_idx,tst_val in enumerate(tst_lst):\n",
    "        tst_lst_idx += 1\n",
    "        pattern_occurrence = 0\n",
    "        long_rois_lst = []\n",
    "        short_rois_lst = []\n",
    "        avg_long_roi = 0\n",
    "        avg_short_roi = 0\n",
    "        avg_sm = 0\n",
    "        sm_lst = []\n",
    "        qal_long_db_chunk_vals = []\n",
    "        if(tst_lst_idx%10 == 0) and tst_lst_idx != 0:\n",
    "            completed = round(tst_lst_idx*100/tot, 1)\n",
    "            remaining = round(100-completed)\n",
    "            cur_ts = time()\n",
    "            ts_diff = int(cur_ts - start_ts)\n",
    "            estimated_tm_to_complete = round(int(ts_diff*(tot-tst_lst_idx)/tst_lst_idx), 1)\n",
    "            print(f\"\\rElapsed Time: {format_elapsed_time(ts_diff)} \\t Completed: {completed}% \\tRemaining: {remaining}% \\tETA: {format_elapsed_time(estimated_tm_to_complete)}\", end=\"\")\n",
    "        for ref_idx,ref_val in enumerate(long_db_chunk):\n",
    "            diff = []\n",
    "            if(all_zeros(ref_val)) or (all_zeros(tst_val) or (tst_val == ref_val)):\n",
    "                continue\n",
    "            else:                \n",
    "                for k in range(chunk_size):\n",
    "                    diff.append(abs(tst_val[k] - ref_val[k]))\n",
    "                cntr += 1\n",
    "                sm = round(sum(diff), 2)\n",
    "                if (sm <= sm_threshold):\n",
    "                    # ts_val = ts[tst_lst_idx]\n",
    "                    if tst_val in enter_long_pos:\n",
    "                        pass\n",
    "                    else:\n",
    "                        enter_long_pos.append(tst_val)\n",
    "                    qal_long_db_chunk_vals.append(ref_val)\n",
    "                    sm_lst.append(sm)\n",
    "                    pattern_occurrence += 1\n",
    "                    # sm_lst.append({'sm': sm, 'pattern_id': pattern_id, 'long_roi': long_roi, 'short_roi': short_roi, 'ref': ref_val, 'tst': tst_val})\n",
    "\n",
    "    # enter_long_pos_df = pd.DataFrame(enter_long_pos)\n",
    "    # print('\\nSaving enter_long_pos_df...')\n",
    "    # enter_long_pos_df.to_csv(f'{downloads_path}/enter_long_pos_df.csv', index=False)\n",
    "    return enter_long_pos\n",
    "\n",
    "tst_lst = list(backtest_chunks['ref_chunks'])\n",
    "enter_long_poss = chunk_diff(long_patterns, tst_lst, pc_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1212, -0.0402, -0.1469, -0.1554, 0.0781, -0.0503, 0.1552, 0.0309, 0.1614, -0.1075]\n"
     ]
    }
   ],
   "source": [
    "print(enter_long_poss[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
