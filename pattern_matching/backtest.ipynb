{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from binance.client import Client\n",
    "from binance.enums import *\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from ast import literal_eval\n",
    "\n",
    "downloads_path = str(Path.home() / \"Downloads\")\n",
    "documents_path = str(Path.home() / \"Documents\")\n",
    "\n",
    "backtest_raw_fn = f'{downloads_path}\\\\backtest_raw_hd.csv'\n",
    "\n",
    "def get_client():\n",
    "    fn = f'{documents_path}\\\\key\\\\binance-key.pickle'\n",
    "    with open(fn, 'rb') as handle:\n",
    "        k = pickle.load(handle)\n",
    "    return Client(k['API_KEY'], k['API_SECRET'])\n",
    "\n",
    "def get_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "def download_hd(start_timestamp, end_timestamp, candlestick): \n",
    "    client = get_client()\n",
    "    data = []\n",
    "    tot = (end_timestamp - start_timestamp)/(900*500)\n",
    "    cntr = 0\n",
    "    for current_sts in range(start_timestamp, end_timestamp+1, 900*500):\n",
    "        next_ets = current_sts + 900*500 if (current_sts + 900*500) < end_timestamp else end_timestamp\n",
    "        print(current_sts, next_ets, f'100% completed') if next_ets == end_timestamp else print(current_sts, next_ets, f'{round(cntr*100/tot, 1)}% completed')\n",
    "        cntr += 1\n",
    "        \n",
    "        klines = client.futures_historical_klines('BTCUSDT', candlestick, current_sts*1000, next_ets*1000, limit=500)\n",
    "        \n",
    "        for kline in klines:\n",
    "            timestamp = kline[0]/1000\n",
    "            open_price = float(kline[1])\n",
    "            high_price = float(kline[2])\n",
    "            low_price = float(kline[3])\n",
    "            close_price = float(kline[4])\n",
    "            volume = float(kline[5])\n",
    "\n",
    "            data.append([timestamp, open_price, high_price, low_price, close_price, volume])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    df.to_csv(backtest_raw_fn, index=False)\n",
    "    print('Historical Data Exported!\\t', backtest_raw_fn)\n",
    "    return df\n",
    "\n",
    "def read_local_hd(fn):\n",
    "    return pd.read_csv(fn)\n",
    "\n",
    "def get_chunks(df):\n",
    "    lst = list(df['close_pc'])\n",
    "    r=[]\n",
    "    tot = len(lst)\n",
    "    for idx, val in enumerate(lst):\n",
    "        if idx < chunk_size:\n",
    "            r.append([0]*chunk_size)\n",
    "        else:\n",
    "            r.append(lst[idx-chunk_size:idx])\n",
    "    df['raw_chunks'] = r\n",
    "    # df.to_csv(f'{downloads_path}/raw_chunks.csv', index=False)\n",
    "    # print('Raw Chunks Exported!', f'{downloads_path}/raw_chunks.csv')\n",
    "    return df\n",
    "\n",
    "def get_pc(hd_df):\n",
    "    close_price = list(hd_df['close'])\n",
    "    ts = list(hd_df['time'])\n",
    "    pc = []\n",
    "    for idx, val in enumerate(close_price):\n",
    "        if idx==0:\n",
    "            pc.append({'time': ts[0], 'pc': 0})\n",
    "        else:\n",
    "            previous_price = close_price[idx-1]\n",
    "            current_price = val\n",
    "            pc_val = (round((current_price*100/previous_price)-100, 4))\n",
    "            pc.append({'time': ts[idx], 'pc': pc_val})\n",
    "    pc_df = pd.DataFrame(pc)\n",
    "    return pc_df\n",
    "\n",
    "\n",
    "def get_tst_chunks():\n",
    "    hd = read_local_hd(backtest_raw_fn)\n",
    "    close_pc_df = get_pc(hd)\n",
    "    hd['close_pc'] = list(close_pc_df['pc'])\n",
    "    chunks_df = get_chunks(hd)\n",
    "    tst_chunks_df = pd.DataFrame()\n",
    "    tst_chunks_df['ts'] = list(chunks_df['time'])\n",
    "    tst_chunks_df['tst_chunks'] = list(chunks_df['raw_chunks'])\n",
    "    tst_chunks_df.to_csv(f'{downloads_path}\\\\backtest_tst_chunks.csv', index=False)\n",
    "    print(f'backtest tst Chunks Exported! {downloads_path}\\\\backtest_tst_chunks.csv')\n",
    "    return tst_chunks_df\n",
    "\n",
    "def save_dict_to_pickle(dictionary, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(dictionary, file)\n",
    "        print(f'Saved {filename}')\n",
    "\n",
    "def load_dict_from_pickle(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        dictionary = pickle.load(file)\n",
    "    print(f'Reading {filename}')\n",
    "    return dictionary\n",
    "\n",
    "def format_ref(r_pkl):\n",
    "    raw_ref_chunks_df = pd.read_csv(f'{downloads_path}/ref_chunks.csv')\n",
    "    raw_ref_ts = list(raw_ref_chunks_df['ts'])\n",
    "    raw_ref_chunk = list(raw_ref_chunks_df['ref_chunks'])\n",
    "    raw_long_qualified = list(raw_ref_chunks_df['long_qualified'])\n",
    "    raw_short_qualified = list(raw_ref_chunks_df['short_qualified'])\n",
    "\n",
    "    formatted_ref = []\n",
    "    for val in r_pkl:\n",
    "        ref_ts = val['ref_ts']\n",
    "        raw_ref_ts_idx = raw_ref_ts.index(ref_ts)\n",
    "\n",
    "        r = {\n",
    "            'avg_avg_dissimilarity': val['avg_avg_dissimilarity'],\n",
    "            'pattern_occurrence': val['pattern_occurrence'],\n",
    "            'ref_ts': ref_ts,\n",
    "            'ref_chunks': literal_eval(raw_ref_chunk[raw_ref_ts_idx]),\n",
    "            'long_qualified': raw_long_qualified[raw_ref_ts_idx],\n",
    "            'short_qualified': raw_short_qualified[raw_ref_ts_idx]\n",
    "        }\n",
    "        formatted_ref.append(r)\n",
    "    return formatted_ref\n",
    "\n",
    "def format_elapsed_time(seconds):\n",
    "    days = seconds // (24 * 3600)\n",
    "    seconds %= 24 * 3600\n",
    "    hours = seconds // 3600\n",
    "    seconds %= 3600\n",
    "    minutes = seconds // 60\n",
    "    seconds %= 60\n",
    "    return f\"{days:03}d:{hours:02}h:{minutes:02}m:{seconds:02}s\"\n",
    "\n",
    "def all_zeros(lst):\n",
    "    return all(x == 0 for x in lst)\n",
    "\n",
    "def get_percent_dissimilarity(ref_lst_chunk, tst_lst_chunk):\n",
    "    percent_dissimilarity = []\n",
    "    for idx in range(len(ref_lst_chunk)):\n",
    "        if ref_lst_chunk[idx] != 0:\n",
    "            percent_dissimilarity.append(abs(round((tst_lst_chunk[idx]*100/ref_lst_chunk[idx])-100, 4)))\n",
    "        else:\n",
    "            percent_dissimilarity.append(9999)\n",
    "    avg = round(sum(percent_dissimilarity)/len(percent_dissimilarity), 4)\n",
    "    return avg\n",
    "\n",
    "def measure_dissimilarity(ref_chunks_lst_dict, tst_chunks_lst_dict):\n",
    "    r= []\n",
    "    qualified_tst_ts_lst = []\n",
    "    qualified_avg_dissimilarity_lst = []\n",
    "    long_rois_lst = []\n",
    "    short_rois_lst = []\n",
    "    start_ts = time()\n",
    "    tot = len(ref_chunks_lst_dict)\n",
    "    backtest_raw_df = pd.read_csv(backtest_raw_fn)\n",
    "    raw_ts = list(backtest_raw_df['time'])\n",
    "    high_price = list(backtest_raw_df['high'])\n",
    "    low_price = list(backtest_raw_df['low'])\n",
    "    close_price = list(backtest_raw_df['close'])\n",
    "    tst_tot = len(raw_ts)\n",
    "    for ref_idx, ref_val in enumerate(ref_chunks_lst_dict):\n",
    "        # if(ref_idx%10 == 0) and ref_idx != 0:\n",
    "        if ref_idx != 0:\n",
    "            completed = round(ref_idx*100/tot, 1)\n",
    "            remaining = round(100-completed)\n",
    "            cur_ts = time()\n",
    "            ts_diff = int(cur_ts - start_ts)\n",
    "            estimated_tm_to_complete = round(int(ts_diff*(tot-ref_idx)/ref_idx), 1)\n",
    "            print(f\"\\rElapsed Time: {format_elapsed_time(ts_diff)} \\t Completed: {completed}% \\tRemaining: {remaining}% \\tETA: {format_elapsed_time(estimated_tm_to_complete)}                               \", end=\"\")\n",
    "        for tst_idx, tst_val in enumerate(tst_chunks_lst_dict):\n",
    "            if ((all_zeros(ref_val['ref_chunks'])) or (all_zeros(tst_val['tst_chunks'])) or (tst_val['tst_chunks'] == ref_val['ref_chunks'])):\n",
    "                continue\n",
    "            else:\n",
    "                avg_dissimilarity = get_percent_dissimilarity(ref_val['ref_chunks'], tst_val['tst_chunks'])\n",
    "                if (avg_dissimilarity <= dissimilarity_threshold) and ((tst_idx + chunk_size + 1) <= tst_tot): \n",
    "                    idx = raw_ts.index(tst_val['ts'])\n",
    "                    cp = close_price[idx]\n",
    "                    max_price = max(high_price[(idx+1):idx + chunk_size])\n",
    "                    min_price = min(low_price[(idx+1):idx + chunk_size])\n",
    "                    long_roi = round((max_price*100/cp)-100, 4)\n",
    "                    short_roi = round((cp*100/min_price)-100, 4)\n",
    "                    long_rois_lst.append(long_roi)\n",
    "                    short_rois_lst.append(short_roi)\n",
    "                    qualified_tst_ts_lst.append(tst_val['ts'])\n",
    "                    qualified_avg_dissimilarity_lst.append(avg_dissimilarity)\n",
    "\n",
    "        pattern_occurrence = len(qualified_avg_dissimilarity_lst)\n",
    "        if (pattern_occurrence >= min_pattern_occurrence):\n",
    "            avg_avg_dissimilarity = round(sum(qualified_avg_dissimilarity_lst)/pattern_occurrence, 4)\n",
    "            ref_position_type = 'long' if ref_val['long_qualified'] else 'short'\n",
    "            avg_pnl = 0\n",
    "            if ref_position_type == 'long':\n",
    "                avg_pnl = round(sum(long_rois_lst)/len(long_rois_lst), 2)\n",
    "            else:\n",
    "                avg_pnl = round(sum(short_rois_lst)/len(short_rois_lst), 2)\n",
    "\n",
    "            r.append({\n",
    "                'avg_avg_dissimilarity': avg_avg_dissimilarity,\n",
    "                'pattern_occurrence': pattern_occurrence,\n",
    "                'avg_pnl': avg_pnl,\n",
    "                'ref_ts': ref_val['ref_ts'],\n",
    "                'qualified_tst_ts_lst': qualified_tst_ts_lst,\n",
    "                'qualified_avg_dissimilarity_lst': qualified_avg_dissimilarity_lst,\n",
    "                'ref_position_type': ref_position_type,\n",
    "                'long_rois_lst': long_rois_lst,\n",
    "                'short_rois_lst': short_rois_lst,\n",
    "            })\n",
    "            save_dict_to_pickle(r, f'{downloads_path}\\\\backtest_r.pkl')\n",
    "        qualified_tst_ts_lst = []\n",
    "        qualified_avg_dissimilarity_lst = [] \n",
    "        long_rois_lst = []\n",
    "        short_rois_lst = []\n",
    "    print('\\nExporting backtest_analyze.csv')\n",
    "    dissimilarity_df = pd.DataFrame(r)\n",
    "    dissimilarity_df.to_csv(f'{downloads_path}\\\\backtest_analyze.csv', index=False)\n",
    "    print('backtest_analyze.csv Exported!')\n",
    "    return dissimilarity_df\n",
    "\n",
    "start_timestamp = 1672516800 #  January 1, 2023 12:00:00 AM GMT+04:00\n",
    "end_timestamp = 1722988800 #  August 7, 2024 4:00:00 AM GMT+04:00\n",
    "\n",
    "\n",
    "config = get_config('config.json')\n",
    "candlestick = config['candlestick']\n",
    "chunk_size = config['chunk_size']\n",
    "roi_threshold = config['roi_threshold']\n",
    "sm_threshold = config['sm_threshold']\n",
    "min_pattern_occurrence = config['min_pattern_occurrence']\n",
    "dissimilarity_threshold = config['dissimilarity_threshold']\n",
    "\n",
    "# download_hd(start_timestamp, end_timestamp, candlestick)\n",
    "hd = read_local_hd(backtest_raw_fn)\n",
    "tst_chunks_df = get_tst_chunks()\n",
    "tst_chunks_lst_dict = tst_chunks_df.to_dict(orient='records')\n",
    "\n",
    "raw_ref = load_dict_from_pickle(f'{downloads_path}/r.pkl')\n",
    "refs = format_ref(raw_ref)[7:30]\n",
    "\n",
    "dissimilarity = measure_dissimilarity(refs, tst_chunks_lst_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
