{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5\n",
      "2\n",
      "3\n",
      "5\n",
      "3\n",
      "9\n",
      "2\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "for i in range(1,11):\n",
    "    print(randint(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_ref = [0, 25, -60, 400, -20, -75, 0, 300, -62.5, 66.7, 0, 40, -42.9, -25, -66.7, 0, 0, 700, -12.5, -28.6]\n",
    "pc_tst = [0, 400, -60, 400, -20, -75, 0, 0, 300, 0]\n",
    "\n",
    "def get_chunks(lst):\n",
    "    r=[]\n",
    "    chunk_size = 3\n",
    "    tot = len(lst)\n",
    "    for idx, val in enumerate(lst):\n",
    "        end_idx = idx+chunk_size\n",
    "        if end_idx <= tot:\n",
    "            r.append(lst[idx:end_idx])\n",
    "    return r\n",
    "\n",
    "ref_chunks = get_chunks(pc_ref)\n",
    "tst_chunks = get_chunks(pc_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_ref = [0.252, 0.3061, 0.2328, 1.4866, -0.0304, -0.2396, 0.2776, -0.559, 0.1776, -0.0879, -0.2489, 0.191, -0.1669, 0.2069, -0.0082, 0.0303, 0.1514, -0.5416, 0.1393, -0.279, 0.1289, 0.0536, 0.0173, 0.0729, -0.3921, 0.331, -0.0669, -0.8499, 0.3064, 0.1968, 0.0437, -0.2584, 0.0821, -0.1622, 0.0575, 0.4276, 0.1731, 0.1696, -0.0697, 0.2628, -0.161, 0.1347, -0.1344, 0.2343, 0.2603, -0.3267, 0.1155, -0.1771, 0.6486, 0.1485, -0.9008, 0.5201, 0.0352, 0.0058, 0.2433, -0.0338, -0.1182, -0.1072, -0.2131, 0.0788, -0.2011, -0.5752, 0.2088, 0.1272, 0.1737, 0.0688, 0.0179, 0.0266, -0.1226, -0.3593, -0.3145, 0.4753, -0.6577, -0.4921, 0.6385, -0.1851, 0.1173, 0.1288, -0.1917, 0.466, 0.1443, -0.1435, 0.1033, -0.033, 0.1452, -0.041, -0.1955, 0.2475, 0.1176, 0.1048, -0.1507, -0.0141, 0.0763, 0.0129, 0.1513, -0.0414, -0.9417, 0.562, 0.0129, -0.291, 0.3272, 0.497, 0.0291, 0.1116, 0.07, -0.1977, -0.096, 0.1428, -0.1951, -0.1208, 0.2339, -0.3064, 0.1438, 0.1687, -0.011, 0.2247, 0.2931, 0.0273, 0.3568, 0.1841, 0.0173, -0.2452, 0.3968, 0.2555, -0.2334, 0.0639, 0.0635, 0.0594, -0.1268, 0.1941, 0.1845, -0.3609, -0.1249, 0.227, -0.5049, 0.2084, 0.346, -0.1565, -0.1739, 0.1898, -0.2511, 0.162, 0.1262, 0.4644, 1.4146, 1.3239, -0.2552, 0.5043, 0.2696, 1.3639, 0.9492, -0.6752, -0.8053, 0.3686, 0.224, -0.0946, 0.3022, 0.9325, 0.0764, 1.1665, 0.8176, 0.8777, -0.2752, 0.3193, -0.0129, 0.2201, 0.5784, -0.7921, 0.5223, 0.0881, 0.7826, -0.1728, 0.215, 0.1403, -0.1354, -0.7929, -1.6224, -0.4311, -3.9419, 2.0467, 0.3975, -0.0158, 0.1819, -0.068, 1.6716, -0.1723, -0.0134, -0.816, 0.283, -0.1722, 0.8016, -0.1964, 0.3173, 0.6438, 0.1002, -0.1255, 0.6785, -0.0524, -0.0911, 0.1621, 0.4624, 0.1326, -0.2818, -0.4339, 0.1541]\n",
    "pc_tst = [0.1776, -0.0879, -0.2489, 0.191, -0.1669, 0.2069, -0.0082, 0.0303, 0.1514, -0.5416]\n",
    "\n",
    "def get_chunks(lst):\n",
    "    r=[]\n",
    "    chunk_size = 10\n",
    "    tot = len(lst)\n",
    "    for idx, val in enumerate(lst):\n",
    "        end_idx = idx+chunk_size\n",
    "        if end_idx <= tot:\n",
    "            r.append(lst[idx:end_idx])\n",
    "    return r\n",
    "\n",
    "ref_chunks = get_chunks(pc_ref)\n",
    "tst_chunks = get_chunks(pc_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tst_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.252, 0.3061, 0.2328, 1.4866, -0.0304, -0.2396, 0.2776, -0.559, 0.1776, -0.0879], [0.3061, 0.2328, 1.4866, -0.0304, -0.2396, 0.2776, -0.559, 0.1776, -0.0879, -0.2489], [0.2328, 1.4866, -0.0304, -0.2396, 0.2776, -0.559, 0.1776, -0.0879, -0.2489, 0.191], [1.4866, -0.0304, -0.2396, 0.2776, -0.559, 0.1776, -0.0879, -0.2489, 0.191, -0.1669], [-0.0304, -0.2396, 0.2776, -0.559, 0.1776, -0.0879, -0.2489, 0.191, -0.1669, 0.2069], [-0.2396, 0.2776, -0.559, 0.1776, -0.0879, -0.2489, 0.191, -0.1669, 0.2069, -0.0082], [0.2776, -0.559, 0.1776, -0.0879, -0.2489, 0.191, -0.1669, 0.2069, -0.0082, 0.0303], [-0.559, 0.1776, -0.0879, -0.2489, 0.191, -0.1669, 0.2069, -0.0082, 0.0303, 0.1514], [0.1776, -0.0879, -0.2489, 0.191, -0.1669, 0.2069, -0.0082, 0.0303, 0.1514, -0.5416], [-0.0879, -0.2489, 0.191, -0.1669, 0.2069, -0.0082, 0.0303, 0.1514, -0.5416, 0.1393], [-0.2489, 0.191, -0.1669, 0.2069, -0.0082, 0.0303, 0.1514, -0.5416, 0.1393, -0.279], [0.191, -0.1669, 0.2069, -0.0082, 0.0303, 0.1514, -0.5416, 0.1393, -0.279, 0.1289], [-0.1669, 0.2069, -0.0082, 0.0303, 0.1514, -0.5416, 0.1393, -0.279, 0.1289, 0.0536], [0.2069, -0.0082, 0.0303, 0.1514, -0.5416, 0.1393, -0.279, 0.1289, 0.0536, 0.0173], [-0.0082, 0.0303, 0.1514, -0.5416, 0.1393, -0.279, 0.1289, 0.0536, 0.0173, 0.0729], [0.0303, 0.1514, -0.5416, 0.1393, -0.279, 0.1289, 0.0536, 0.0173, 0.0729, -0.3921], [0.1514, -0.5416, 0.1393, -0.279, 0.1289, 0.0536, 0.0173, 0.0729, -0.3921, 0.331], [-0.5416, 0.1393, -0.279, 0.1289, 0.0536, 0.0173, 0.0729, -0.3921, 0.331, -0.0669]]\n",
      "[[0.1776, -0.0879, -0.2489, 0.191, -0.1669, 0.2069, -0.0082, 0.0303, 0.1514, -0.5416]]\n"
     ]
    }
   ],
   "source": [
    "print(ref_chunks)\n",
    "print(tst_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def chunk_diff(ref_lst, tst_lst):\n",
    "    cntr = 0\n",
    "    chunk_size = len(tst_lst[0])\n",
    "    sm_lst = []\n",
    "    sm_idx_dic = {}\n",
    "    print(chunk_size)\n",
    "    for i in tst_lst:\n",
    "        for j in ref_lst:\n",
    "            diff = []\n",
    "            for k in range(chunk_size):\n",
    "                diff.append(abs(i[k] - j[k]))\n",
    "            cntr += 1\n",
    "            sm = sum(diff)\n",
    "            # sm_lst.append({'k': f'{cntr}: {i} vs {j}', 'sm': sm})\n",
    "            sm_lst.append({'k': cntr, 'sm': sm})\n",
    "    return sm_lst\n",
    "\n",
    "sm_lst = chunk_diff(ref_chunks, tst_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 9, 'sm': 0.0} {'k': 176, 'sm': 11.8161}\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def sort_dicts_by_value(dicts: List[Dict], key: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Sorts a list of dictionaries by the specified key value.\n",
    "\n",
    "    :param dicts: List of dictionaries to be sorted\n",
    "    :param key: The key by which the dictionaries should be sorted\n",
    "    :return: A list of dictionaries sorted by the specified key\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return sorted(dicts, key=lambda x: x[key])\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"One or more dictionaries do not have the key '{key}'\")\n",
    "    except TypeError:\n",
    "        raise TypeError(f\"The value for key '{key}' is not comparable\")\n",
    "\n",
    "sorted_list = sort_dicts_by_value(sm_lst, 'sm')\n",
    "print(sorted_list[0],sorted_list[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from binance.client import Client\n",
    "from binance.enums import *\n",
    "from time import time\n",
    "import pickle as pickle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "downloads_path = str(Path.home() / \"Downloads\")\n",
    "documents_path = str(Path.home() / \"Documents\")\n",
    "tst_fn = f'{downloads_path}\\\\hd.csv'\n",
    "hd_dl_fn = f'{downloads_path}\\\\hd_dl.csv'\n",
    "qualifying_trades_fn = f'{downloads_path}\\\\qualifying_trades.csv'\n",
    "\n",
    "\n",
    "def get_client():\n",
    "    fn = f'{documents_path}\\\\key\\\\binance-key.pickle'\n",
    "    with open(fn, 'rb') as handle:\n",
    "        k = pickle.load(handle)\n",
    "    return Client(k['API_KEY'], k['API_SECRET'])\n",
    "\n",
    "def get_config(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_chunks(lst, chunk_size):\n",
    "    r=[]\n",
    "    tot = len(lst)\n",
    "    for idx, val in enumerate(lst):\n",
    "        end_idx = idx+chunk_size\n",
    "        if end_idx <= tot:\n",
    "            r.append(lst[idx:end_idx])\n",
    "    return r\n",
    "\n",
    "def get_pc(hd_df):\n",
    "    close_price = list(hd_df['close'])\n",
    "    ts = list(hd_df['time'])\n",
    "    pc = []\n",
    "    for idx, val in enumerate(close_price):\n",
    "        if idx==0:\n",
    "            pc.append({'time': ts[0], 'pc': 0})\n",
    "        else:\n",
    "            previous_price = close_price[idx-1]\n",
    "            current_price = val\n",
    "            pc_val = (round((current_price*100/previous_price)-100, 4))\n",
    "            pc.append({'time': ts[idx], 'pc': pc_val})\n",
    "    pc_df = pd.DataFrame(pc)\n",
    "    return pc_df\n",
    "\n",
    "def get_trades(hd_df, chunk_size):\n",
    "    close_price = list(hd_df['close'])\n",
    "    high_price = list(hd_df['high'])\n",
    "    low_price = list(hd_df['low'])\n",
    "    longs=[]\n",
    "    shorts=[]\n",
    "    long_qualified = []\n",
    "    short_qualified = []\n",
    "    tot = len(close_price)\n",
    "    for idx in range(len(close_price)):\n",
    "        if (idx + chunk_size) <= tot:\n",
    "            cp = close_price[idx]\n",
    "            max_price = max(high_price[idx:idx + chunk_size])\n",
    "            min_price = min(low_price[idx:idx + chunk_size])\n",
    "            long_roi = round((max_price*100/cp)-100, 4)\n",
    "            short_roi = round((cp*100/min_price)-100, 4)\n",
    "            longs.append({'long': long_roi})\n",
    "            long_qualified.append(long_roi >= roi_threshold)\n",
    "            shorts.append({'short': short_roi})\n",
    "            short_qualified.append(short_roi >= roi_threshold)\n",
    "        else:\n",
    "            long_roi = 0.0\n",
    "            short_roi = 0.0\n",
    "            longs.append({'long': long_roi})\n",
    "            long_qualified.append(False)\n",
    "            shorts.append({'short': short_roi})\n",
    "            short_qualified.append(False)\n",
    "    long_trades_df = pd.DataFrame(longs)\n",
    "    short_trades_df = pd.DataFrame(shorts)\n",
    "    long_qualified_df = pd.DataFrame(long_qualified)\n",
    "    short_qualified_df = pd.DataFrame(short_qualified)\n",
    "    return long_trades_df, short_trades_df, long_qualified, short_qualified\n",
    "\n",
    "def download_hd(start_timestamp, end_timestamp, candlestick): \n",
    "    client = get_client()\n",
    "    data = []\n",
    "    tot = (end_timestamp - start_timestamp)/(900*500)\n",
    "    cntr = 0\n",
    "    for current_sts in range(start_timestamp, end_timestamp+1, 900*500):\n",
    "        next_ets = current_sts + 900*500 if (current_sts + 900*500) < end_timestamp else end_timestamp\n",
    "        print(current_sts, next_ets, f'100% completed') if next_ets == end_timestamp else print(current_sts, next_ets, f'{round(cntr*100/tot, 1)}% completed')\n",
    "        cntr += 1\n",
    "        \n",
    "        klines = client.futures_historical_klines('BTCUSDT', candlestick, current_sts*1000, next_ets*1000, limit=500)\n",
    "        \n",
    "        for kline in klines:\n",
    "            timestamp = kline[0]/1000\n",
    "            open_price = float(kline[1])\n",
    "            high_price = float(kline[2])\n",
    "            low_price = float(kline[3])\n",
    "            close_price = float(kline[4])\n",
    "            volume = float(kline[5])\n",
    "\n",
    "            data.append([timestamp, open_price, high_price, low_price, close_price, volume])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    df.to_csv(hd_dl_fn, index=False)\n",
    "    print('Historical Data Exported!\\t', hd_dl_fn)\n",
    "    return df\n",
    "\n",
    "def read_local_hd(fn):\n",
    "    return pd.read_csv(fn)\n",
    "\n",
    "def get_qualifying_trades(df):\n",
    "    close_pc_df = get_pc(df)\n",
    "    long_trades_df, short_trades_df, long_qualified, short_qualified = get_trades(df, chunk_size)\n",
    "    df['close_pc'] = close_pc_df['pc']\n",
    "    df['long'] = long_trades_df\n",
    "    df['short'] = short_trades_df\n",
    "    df['long_qualified'] = long_qualified\n",
    "    df['short_qualified'] = short_qualified\n",
    "    df.to_csv(qualifying_trades_fn, index=False)\n",
    "    print('Qualifying Trades Data Exported!', qualifying_trades_fn)\n",
    "    return df\n",
    "\n",
    "def get_chunks(qt_df):\n",
    "    lst = list(qt_df['close_pc'])\n",
    "    r=[]\n",
    "    tot = len(lst)\n",
    "    for idx, val in enumerate(lst):\n",
    "        if idx < chunk_size:\n",
    "            r.append([0]*chunk_size)\n",
    "        else:\n",
    "            r.append(lst[idx-chunk_size:idx])\n",
    "    qt_df['ref_chunks'] = r\n",
    "    qt_df.to_csv(f'{downloads_path}/ref_chunks.csv', index=False)\n",
    "    print('Ref Chunks Exported!', f'{downloads_path}/ref_chunks.csv')\n",
    "    return qt_df\n",
    "\n",
    "def get_chunk_diff(ref_lst, tst_lst):\n",
    "    cntr = 0\n",
    "    sm_lst = []\n",
    "    sm_idx_dic = {}\n",
    "    tot = len(ref_lst)\n",
    "    for idx,i in enumerate(tst_lst):\n",
    "        if (idx % 100 == 0) and (idx > 0):\n",
    "            rem = round((idx*100/tot)-100, 2)\n",
    "            print(rem, '%', idx)\n",
    "        for j in ref_lst:\n",
    "            diff = []\n",
    "            for k in range(chunk_size):\n",
    "                diff.append(abs(i[k] - j[k]))\n",
    "            cntr += 1\n",
    "            sm = sum(diff)\n",
    "            # sm_lst.append({'k': f'{cntr}: {i} vs {j}', 'sm': sm})\n",
    "            sm_lst.append(sm)\n",
    "    return sm_lst\n",
    "\n",
    "def get_tst_chunks(ref_chunks_df):\n",
    "    long_qualified = list(ref_chunks_df['long_qualified'])\n",
    "    short_qualified = list(ref_chunks_df['short_qualified'])\n",
    "    ref_chunks_lst = list(ref_chunks_df['ref_chunks'])\n",
    "    r=[]\n",
    "    tst_lst = []\n",
    "    chunk_diff_lst = []\n",
    "    tst_lst_samp = [0.0058, 0.2433, -0.0338, -0.1182, -0.1072, -0.2131, 0.0788, -0.2011, -0.5752, 0.2088]\n",
    "    for idx in range(len(long_qualified)):\n",
    "        if (idx < chunk_size) or ((long_qualified[idx] == False) and (short_qualified[idx] == False)) :\n",
    "            r.append(None)\n",
    "            chunk_diff_lst.append(None)\n",
    "        else:\n",
    "            if (long_qualified[idx] == True) or (short_qualified[idx] == True):\n",
    "                r.append(ref_chunks_lst[idx])\n",
    "                tst_lst.append(ref_chunks_lst[idx])\n",
    "                chunk_diff_lst.append(get_chunk_diff(ref_chunks_lst, [tst_lst_samp]))\n",
    "    ref_chunks_df['tst_chunks'] = r\n",
    "    ref_chunks_df['sm'] = chunk_diff_lst\n",
    "    ref_chunks_df.to_csv(f'{downloads_path}/tst_chunks.csv', index=False)\n",
    "    print('TST Chunks Exported!', f'{downloads_path}/tst_chunks.csv')\n",
    "    return ref_chunks_df, tst_lst\n",
    "\n",
    "config = get_config('config.json')\n",
    "candlestick = config['candlestick']\n",
    "chunk_size = config['chunk_size']\n",
    "roi_threshold = config['roi_threshold']\n",
    "sm_threshold = config['sm_threshold']\n",
    "\n",
    "start_timestamp = 1609459200 #  January 1, 2021 12:00:00 AM\n",
    "end_timestamp = 1719721304\n",
    "\n",
    "# ref_chunks = get_chunks(pc_ref)\n",
    "# tst_chunks = get_chunks(pc_tst)\n",
    "hd = read_local_hd(hd_dl_fn)\n",
    "qt_df = get_qualifying_trades(hd)\n",
    "\n",
    "ref_chunks_df = get_chunks(qt_df)\n",
    "tst_chunks_df, tst_lst = get_tst_chunks(ref_chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122514"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ref_chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50760"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tst_lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
