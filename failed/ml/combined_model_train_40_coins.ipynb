{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "from binance.client import Client\n",
    "from time import time\n",
    "import pickle as pickle\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "from pycaret.classification import load_model, predict_model\n",
    "from pycaret.classification import *\n",
    "from pycaret.classification import ClassificationExperiment\n",
    "from lightgbm import LGBMClassifier\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "def get_client():\n",
    "    fn = '../../key/binance-key.pickle'\n",
    "    # fn = '/home/era/key/binance-key.pickle'\n",
    "    with open(fn, 'rb') as handle:\n",
    "        k = pickle.load(handle)\n",
    "    return Client(k['API_KEY'], k['API_SECRET'])\n",
    "\n",
    "\n",
    "client = get_client()\n",
    "\n",
    "\n",
    "def get_unix_timestamp(date_string):\n",
    "    \"\"\"\n",
    "    Converts the input date string to Unix timestamp.\n",
    "\n",
    "    Parameters:\n",
    "        date_string (str): Input date string in the format \"dd/mm/yyyy hh:mm:ss\".\n",
    "\n",
    "    Returns:\n",
    "        int: Unix timestamp of the given date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        date_obj = datetime.strptime(date_string, \"%d/%m/%Y %H:%M:%S\")\n",
    "        timestamp = int(date_obj.timestamp())\n",
    "        return timestamp\n",
    "    except ValueError:\n",
    "        print(\"Invalid date format. Please use the format 'dd/mm/yyyy hh:mm:ss'.\")\n",
    "        return None\n",
    "\n",
    "def get_historical_data(start_timestamp, end_timestamp, coin_pair): \n",
    "    data = []\n",
    "    tot = (end_timestamp - start_timestamp)/(900*500)\n",
    "    cntr = 0\n",
    "    for current_sts in range(start_timestamp, end_timestamp+1, 900*500):\n",
    "        next_ets = current_sts + 900*500 if (current_sts + 900*500) < end_timestamp else end_timestamp\n",
    "        print(current_sts, next_ets, f'100% completed') if next_ets == end_timestamp else print(current_sts, next_ets, f'{round(cntr*100/tot, 1)}% completed')\n",
    "        cntr += 1\n",
    "        # Futures market\n",
    "        klines = client.futures_historical_klines(coin_pair, '15m', current_sts*1000, next_ets*1000, limit=500)\n",
    "        # Spot market\n",
    "        # klines = client.get_historical_klines(coin_pair, interval, current_sts*1000, next_ets*1000, limit=500)\n",
    "        \n",
    "        for kline in klines:\n",
    "            timestamp = kline[0]/1000\n",
    "            open_price = float(kline[1])\n",
    "            high_price = float(kline[2])\n",
    "            low_price = float(kline[3])\n",
    "            close_price = float(kline[4])\n",
    "            volume = float(kline[5])\n",
    "\n",
    "            data.append([timestamp, open_price, high_price, low_price, close_price, volume])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['time', 'open', 'high', 'low', 'close', 'volume'])\n",
    "    # df.to_csv(f'/home/ubuntu/data/{coin_pair}-{interval}.csv', index=False)\n",
    "    # print('Data Exported')\n",
    "    print(f'Historical Data of {coin_pair} Downloaded')\n",
    "    return df\n",
    "\n",
    "def generate_features(start_timestamp, end_timestamp, coin_pair):\n",
    "    df = get_historical_data(start_timestamp, end_timestamp, coin_pair)\n",
    "    candlestick_frame = 12\n",
    "    pnl_threshold = 3\n",
    "\n",
    "\n",
    "    # df = pd.read_csv('/home/ubuntu/data/ETHUSDT-15m.csv')\n",
    "    try:\n",
    "        df.ta.strategy(\"all\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ts = list(df['time'])\n",
    "        open = list(df['open'])\n",
    "        high = list(df['high'])\n",
    "        low = list(df['low'])\n",
    "        close = list(df['close'])\n",
    "        volume = list(df['volume'])\n",
    "        tot = len(ts)\n",
    "        long_runup_lst = []\n",
    "        long_drawdown_lst = []\n",
    "        short_runup_lst = []\n",
    "        short_drawdown_lst = []\n",
    "\n",
    "        for idx in range(tot):\n",
    "            if (idx >= candlestick_frame) and (idx <= tot - candlestick_frame):\n",
    "                max_high = max(high[idx+1:idx+candlestick_frame])\n",
    "                min_low = min(low[idx+1:idx+candlestick_frame])\n",
    "                entry_price = open[idx+1]\n",
    "                long_runup_lst.append(round((max_high*100/entry_price)-100, 6))\n",
    "                long_drawdown_lst.append(round((min_low*100/entry_price)-100, 6))\n",
    "                short_runup_lst.append(round((entry_price*100/min_low)-100, 6))\n",
    "                short_drawdown_lst.append(round((entry_price*100/max_high)-100, 6))\n",
    "            else:\n",
    "                long_runup_lst.append(0)\n",
    "                long_drawdown_lst.append(0)\n",
    "                short_runup_lst.append(0)\n",
    "                short_drawdown_lst.append(0)     \n",
    "\n",
    "\n",
    "        long=[]\n",
    "        short=[]\n",
    "        dont_trade=[]\n",
    "        signal = []\n",
    "\n",
    "        for idx in range(tot):\n",
    "            if (idx >= candlestick_frame) and (idx <= tot - candlestick_frame):\n",
    "                if long_runup_lst[idx] >= pnl_threshold:\n",
    "                    signal.append('long')\n",
    "                elif short_runup_lst[idx] >= pnl_threshold:\n",
    "                    signal.append('short')\n",
    "                else:\n",
    "                    signal.append('dont_trade')\n",
    "            else:\n",
    "                signal.append('dont_trade')\n",
    "\n",
    "        df['coin'] = [coin_pair]*len(signal)\n",
    "        df['signal'] = signal\n",
    "\n",
    "        long_indices = df[df['signal'].str.contains('long', case=False)].index\n",
    "        short_indices = df[df['signal'].str.contains('short', case=False)].index\n",
    "        dont_trade_indices = list(df[df['signal'].str.contains('dont_trade', case=False)].index)\n",
    "        num_indices_to_pick  = len(dont_trade_indices) - min([len(long_indices), len(short_indices)])\n",
    "        random_indices = random.sample(dont_trade_indices, num_indices_to_pick)\n",
    "        df = df.drop(random_indices)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        df = df.drop(columns = ['time', 'open', 'high', 'low', 'close', 'volume'], axis=1)\n",
    "        dataset_fn = f'../../data/{coin_pair}-dataset.csv'\n",
    "        pd.DataFrame(df).to_csv(dataset_fn, index=False)\n",
    "        print(f'{coin_pair} Features Generated and saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins_list = ['BTC', 'ETH', 'BNB', 'XRP', 'ADA', 'DOGE', 'SOL', 'TRX', 'DOT', 'MATIC', 'LTC', 'BCH', 'AVAX', 'XLM', 'LINK', 'UNI', 'XMR', 'ATOM', 'ETC', 'HBAR', 'ICP', 'FIL', 'LDO', 'APT', 'ARB', 'QNT', 'VET', 'NEAR', 'OP', 'MKR', 'GRT', 'AAVE', 'ALGO', 'AXS', 'EGLD', 'STX', 'SAND', 'XTZ', 'EOS', 'INJ', 'THETA', 'IMX', 'SNX', 'MANA', 'FTM', 'RUNE', 'APE', 'RNDR', 'NEO', 'KAVA', 'FLOW', 'CHZ', 'GALA', 'KLAY', 'SUI', 'FXS', 'ZEC', 'CFX', 'CRV', 'MINA', 'COMP', 'GMX', 'DYDX', 'WOO', 'ASTR']\n",
    "for i in range(len(coins_list)):\n",
    "    coin_pair = f'{coins_list[i]}USDT'\n",
    "    print('working on', coin_pair)\n",
    "    # start_timestamp = get_unix_timestamp('1/1/2016 00:00:00')\n",
    "    start_timestamp = get_unix_timestamp('1/6/2022 00:00:00')\n",
    "    end_timestamp = int(time())\n",
    "    generate_features(start_timestamp, end_timestamp, coin_pair)\n",
    "    \n",
    "coin_feaures = [f'{val}USDT-dataset.csv' for val in coins_list]\n",
    "dfs=[]\n",
    "dataset_path = '../../data/'\n",
    "\n",
    "for fn in coin_feaures:\n",
    "    dfs.append(pd.read_csv(f'{dataset_path}{fn}'))\n",
    "\n",
    "concatenated_df = pd.concat(dfs, ignore_index=True)  # Set ignore_index=True to reset index\n",
    "pd.DataFrame(concatenated_df).to_csv(f'{dataset_path}combined_dataset.csv', index=False)\n",
    "\n",
    "exp = ClassificationExperiment()\n",
    "dataset_path = '../../data/'\n",
    "data = pd.read_csv(f'{dataset_path}combined_dataset.csv')\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "s = setup(data, target = 'signal', categorical_features=['coin'], session_id = 123, use_gpu=True)\n",
    "model = create_model(LGBMClassifier())\n",
    "validation_scores = pull()\n",
    "accuracy_mean = validation_scores['Accuracy']['Mean']\n",
    "\n",
    "# save pipeline\n",
    "model_name = 'combined_model'\n",
    "save_model(model, f'../../models/{model_name}-{accuracy_mean}')\n",
    "print(f'{model_name} model saved. accuracy_mean={accuracy_mean}')\n",
    "plot_model(model, plot = 'confusion_matrix', plot_kwargs = {'percent': True})\n",
    "# plot_model(model, plot = 'feature_all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import load_model, predict_model\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../../data/combined_dataset.csv')\n",
    "s = setup(data, target = 'signal', session_id = 123, use_gpu=True)\n",
    "\n",
    "# Load trained Pipeline\n",
    "model_name = 'combined_model-0.9151'\n",
    "model = load_model(f'../../models/{model_name}')\n",
    "plot_model(model, plot = 'confusion_matrix', plot_kwargs = {'percent': True})\n",
    "# plot_model(model, plot = 'feature_all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
